{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "rAO40DsiO3Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "id": "l52crFLShGIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import pdfplumber\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "import warnings\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from statistics import mean\n",
        "from heapq import nlargest\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "xbgbVxl1PFzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "jdxVFm-2Phfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialise the bank that is being processed\n",
        "bank = ''"
      ],
      "metadata": {
        "id": "7VICv-fPPvXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear existing handlers\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "# Configure logging with both console and file output\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"pdf_processing_errors.log\"),\n",
        "        logging.StreamHandler()  # Output to console\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "RgfxmjspO8c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_header_text(text=None):\n",
        "    \"\"\"Remove header text from the extracted text.\"\"\"\n",
        "    cleaned_text = []\n",
        "    for line in text.splitlines():\n",
        "        if len(line) < 5 or re.search(r'', line, re.IGNORECASE):\n",
        "            continue\n",
        "        cleaned_text.append(line)\n",
        "    return \"\\n\".join(cleaned_text)\n",
        "\n",
        "def clean_text(text):\n",
        "    cleaned_text = []\n",
        "    for line in text.splitlines():\n",
        "        if len(line) < 5 or re.search(r'', line, re.IGNORECASE):\n",
        "            #print(line)\n",
        "            continue\n",
        "        cleaned_text.append(line)\n",
        "    return \"\\n\".join(cleaned_text)\n",
        "\n",
        "# PDF Text Extraction Functions\n",
        "def extract_first_page_text(file_path=None):\n",
        "    \"\"\"Extracts text from the first page of a PDF file to find year and quarter information.\"\"\"\n",
        "    try:\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            first_page_text = pdf.pages[0].extract_text()\n",
        "        return first_page_text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in extract_first_page_text, file: {file_path}, error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_year_quarter_from_text(text=None):\n",
        "\n",
        "    \"\"\"Extracts year and quarter information from the text.\"\"\"\n",
        "\n",
        "    match = re.search(r\"(Q[1-4]\\s*\\d{4}|\\d{4}\\s*Q[1-4])\", text)\n",
        "    if match:\n",
        "        found = match.group(0)\n",
        "        if found.startswith(\"Q\"):\n",
        "            quarter, year = found.split()\n",
        "        else:\n",
        "            year, quarter = found.split()\n",
        "        return quarter.strip(), year.strip()\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def extract_company_participants(text):\n",
        "    \"\"\"Extracts company participants from the 'Company Participants' section of the text.\"\"\"\n",
        "    company_participants = []\n",
        "    in_company_section = False\n",
        "\n",
        "    for line in text.splitlines():\n",
        "        if \"Company Participants\" in line:\n",
        "            in_company_section = True\n",
        "            continue\n",
        "\n",
        "        if \"Operator\" in line or \"Question-and-Answer\" in line or \"Conference Call Participants\" in line:\n",
        "            break\n",
        "\n",
        "        if in_company_section:\n",
        "            match = re.match(r'(?P<name>[\\w\\s\\.\\-\\'\\u00C0-\\u017F]+?)\\s*[-–]\\s*(?P<designation>[\\w\\s,&\\.\\-\\'\\u00C0-\\u017F]+)', line)\n",
        "            if match:\n",
        "                name = match.group(\"name\").strip()\n",
        "                designation = match.group(\"designation\").strip()\n",
        "                designation = re.sub(r'\\s*&\\s*', ' and ', designation)\n",
        "                designation = re.sub(r'\\s*,\\s*', ', ', designation)\n",
        "                company_participants.append((name, designation))\n",
        "\n",
        "    return company_participants\n",
        "\n",
        "def extract_conference_participants(text):\n",
        "    \"\"\"Extracts conference participants from the 'Conference Call Participants' section of the text.\"\"\"\n",
        "    conference_participants = []\n",
        "    in_conference_section = False\n",
        "\n",
        "    for line in text.splitlines():\n",
        "        if \"Conference Call Participants\" in line:\n",
        "            in_conference_section = True\n",
        "            continue\n",
        "\n",
        "        if \"Operator\" in line or \"Question-and-Answer\" in line or \"Disclaimer\" in line:\n",
        "            break\n",
        "\n",
        "        if in_conference_section:\n",
        "            match = re.match(r'(?P<name>[\\w\\s\\.\\-\\'\\u00C0-\\u017F]+?)\\s*[-–]\\s*(?P<bank>[\\w\\s,&\\.\\-\\'\\u00C0-\\u017F]+)', line)\n",
        "            if match:\n",
        "                name = match.group(\"name\").strip()\n",
        "                bank = match.group(\"bank\").strip()\n",
        "                conference_participants.append((name, bank))\n",
        "\n",
        "    return conference_participants\n",
        "\n",
        "def extract_participants_from_text(text):\n",
        "    \"\"\"Extracts both company and conference participants from the text.\"\"\"\n",
        "    company_participants = extract_company_participants(text)\n",
        "    conference_participants = extract_conference_participants(text)\n",
        "\n",
        "    return company_participants, conference_participants\n",
        "\n",
        "\n",
        "def extract_qa_section(text):\n",
        "    # Locate the start of the Q&A section and isolate that portion of the text\n",
        "    qa_section_start = text.find(\"Question-and-Answer Session\")\n",
        "    if qa_section_start == -1:\n",
        "        return \"\"\n",
        "    return text[qa_section_start:]\n",
        "\n",
        "def extract_interview_details(text, company_df, conference_df):\n",
        "    # Define regex patterns to capture Q&A pairs sequentially\n",
        "    qa_pattern = r\"\\n([A-Z][a-z]+ [A-Z][a-z]+)\\n(.*?)(?=\\n[A-Z][a-z]+ [A-Z][a-z]+|\\nOperator|\\Z)\"\n",
        "\n",
        "    # Extract all Q&A pairs in sequence\n",
        "    qa_matches = re.findall(qa_pattern, text, re.DOTALL)\n",
        "\n",
        "    # Initialize list to hold Q&A pairs\n",
        "    qa_data = []\n",
        "\n",
        "    # Process each pair as a (question, answer) sequentially\n",
        "    for i in range(0, len(qa_matches) - 1, 2):\n",
        "        interviewer, question = qa_matches[i]\n",
        "        interviewee, answer = qa_matches[i + 1]\n",
        "\n",
        "        # Lookup bank for interviewer from conference_df\n",
        "        interviewer_bank = conference_df.loc[conference_df['Name'] == interviewer, 'Bank'].values\n",
        "        interviewer_bank = interviewer_bank[0] if interviewer_bank.size > 0 else None\n",
        "\n",
        "        # Lookup designation for interviewee from company_df\n",
        "        interviewee_designation = company_df.loc[company_df['Name'] == interviewee, 'Designation'].values\n",
        "        interviewee_designation = interviewee_designation[0] if interviewee_designation.size > 0 else None\n",
        "\n",
        "        # Append structured Q&A data\n",
        "        qa_data.append({\n",
        "            \"Interviewer\": interviewer,\n",
        "            \"Interviewer Bank\": interviewer_bank,\n",
        "            \"Question\": question.strip(),\n",
        "            \"Interviewee\": interviewee,\n",
        "            \"Interviewee Designation\": interviewee_designation,\n",
        "            \"Answer\": answer.strip()\n",
        "        })\n",
        "\n",
        "    # Convert the collected data into a DataFrame\n",
        "    qa_df = pd.DataFrame(qa_data)\n",
        "\n",
        "    # Fill missing values with specified text\n",
        "    qa_df['Interviewer Bank'] = qa_df['Interviewer Bank'].fillna(\"Interviewee\")\n",
        "    qa_df['Interviewee Designation'] = qa_df['Interviewee Designation'].fillna(\"Interviewer\")\n",
        "\n",
        "    return qa_df\n",
        "\n",
        "\n",
        "def process_pdf(file_path):\n",
        "    base_name = os.path.basename(file_path)\n",
        "    text = extract_first_page_text(file_path)\n",
        "    quarter, year = extract_year_quarter_from_text(text)\n",
        "\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "\n",
        "    text = clean_text(text)\n",
        "\n",
        "    # Extract company and conference participants\n",
        "    company_participants, conference_participants = extract_participants_from_text(text)\n",
        "\n",
        "    return base_name, year, quarter, company_participants, conference_participants\n",
        "\n",
        "def process_folder(folder_path):\n",
        "    company_data = []\n",
        "    conference_data = []\n",
        "    document_data = []  # For storing bank and cleaned_text data\n",
        "\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        # Process only PDF files\n",
        "        if file_name.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "            try:\n",
        "                # Process the PDF and unpack returned values\n",
        "                base_name, year, quarter, bank_name, cleaned_text, company_participants, conference_participants = process_pdf(file_path)\n",
        "\n",
        "                # Populate Company Participants Data\n",
        "                for name, designation in company_participants:\n",
        "                    company_data.append({\n",
        "                        \"Year\": year,\n",
        "                        \"Quarter\": quarter,\n",
        "                        \"Name\": name,\n",
        "                        \"Designation\": designation,\n",
        "                        \"FileName\": base_name,\n",
        "                        \"Bank\": bank_name\n",
        "                    })\n",
        "\n",
        "                # Populate Conference Participants Data\n",
        "                for name, bank in conference_participants:\n",
        "                    conference_data.append({\n",
        "                        \"Year\": year,\n",
        "                        \"Quarter\": quarter,\n",
        "                        \"Name\": name,\n",
        "                        \"Bank\": bank,\n",
        "                        \"FileName\": base_name\n",
        "                    })\n",
        "\n",
        "                # Populate Document Data\n",
        "                document_data.append({\n",
        "                    \"Year\": year,\n",
        "                    \"Quarter\": quarter,\n",
        "                    \"Bank\": bank_name,\n",
        "                    \"FileName\": base_name,\n",
        "                    \"Text\": cleaned_text\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                # Log any error that occurs during processing of a specific PDF file\n",
        "                logging.error(f\"Error processing file {file_name} in {folder_path}: {e}\")\n",
        "                continue  # Skip the file with issues and move to the next one\n",
        "\n",
        "    # Convert lists to DataFrames\n",
        "    try:\n",
        "        company_df = pd.DataFrame(company_data, columns=[\"Year\", \"Quarter\", \"Name\", \"Designation\", \"FileName\", \"Bank\"])\n",
        "        conference_df = pd.DataFrame(conference_data, columns=[\"Year\", \"Quarter\", \"Name\", \"Bank\", \"FileName\"])\n",
        "        document_df = pd.DataFrame(document_data, columns=[\"Year\", \"Quarter\", \"Bank\", \"FileName\", \"Text\"])\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error creating DataFrames from processed data: {e}\")\n",
        "        return None, None, None  # Return None if DataFrame creation fails\n",
        "\n",
        "    return company_df, conference_df, document_df\n",
        "\n",
        "\n",
        "def process_pdf_q_and_a(file_path, company_df, conference_df):\n",
        "    base_name = os.path.basename(file_path)\n",
        "    text = extract_first_page_text(file_path)\n",
        "    quarter, year = extract_year_quarter_from_text(text)\n",
        "\n",
        "    is_new_line = is_noise = qa_section = False\n",
        "    text_by = text_type = text_to_add = \"\"\n",
        "    qa_data = []\n",
        "    noises = [\"call-transcript\", \"Call Transcript\"]\n",
        "    key_words = [\"Operator\"]\n",
        "    section = \"Presentation\"\n",
        "\n",
        "    # Open the PDF file\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page_num, page in enumerate(pdf.pages):\n",
        "            # Extract text line-by-line to identify speakers\n",
        "            lines = page.extract_text().split('\\n')\n",
        "\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                is_noise = any(noise in line for noise in noises)\n",
        "\n",
        "                if not is_noise:\n",
        "                    is_new_line, text_by, text_type, text_to_add, section = add_q_and_a(is_new_line, text_by, text_type, text_to_add, qa_data, company_df.Name, line, \"Answer\", section, qa_section)\n",
        "                    is_new_line, text_by, text_type, text_to_add, section = add_q_and_a(is_new_line, text_by, text_type, text_to_add, qa_data, conference_df.Name, line, \"Question\", section, qa_section)\n",
        "                    is_new_line, text_by, text_type, text_to_add, section = add_q_and_a(is_new_line, text_by, text_type, text_to_add, qa_data, key_words, line, \"Operator\", section, qa_section)\n",
        "\n",
        "                    if is_new_line == True:\n",
        "                        is_new_line = False\n",
        "                        text_to_add = \"\"\n",
        "                    else:\n",
        "                        text_to_add += \" \" + line\n",
        "\n",
        "                if \"Question-and-Answer\" in line:\n",
        "                    qa_section = True\n",
        "\n",
        "    qa_data.append({\n",
        "        \"Text Type\": text_type,\n",
        "        \"Name\": text_by,\n",
        "        \"Dialogue\": text_to_add.replace(text_by, '').strip(),\n",
        "        \"Section\":section\n",
        "    })\n",
        "\n",
        "    qa_df = pd.DataFrame(qa_data)\n",
        "\n",
        "    qa_df[\"Year\"] = year\n",
        "    qa_df[\"Quarter\"] = quarter\n",
        "    qa_df[\"FileName\"] = base_name\n",
        "\n",
        "    return qa_df\n",
        "\n",
        "def add_q_and_a(is_new_line, text_by, text_type, text_to_add, qa_data, key_words, line, new_text_type, section, qa_section):\n",
        "    for key_word in key_words:\n",
        "        if line in key_word:\n",
        "            is_new_line = True\n",
        "            if text_to_add.strip():\n",
        "                if text_by:\n",
        "                    qa_data.append({\n",
        "                        \"Text Type\": text_type,\n",
        "                        \"Name\": text_by,\n",
        "                        \"Dialogue\": text_to_add.replace(text_by, '').strip(),\n",
        "                        \"Section\":section\n",
        "                    })\n",
        "                    if qa_section:\n",
        "                        section = \"Question-and-Answer\"\n",
        "                text_to_add = \"\"\n",
        "                is_new_line = False\n",
        "            text_by = line\n",
        "            text_type = new_text_type\n",
        "\n",
        "    return is_new_line, text_by, text_type, text_to_add, section\n",
        "\n",
        "def process_folder_q_and_a(folder_path, company_df, conference_df):\n",
        "    all_qa_data = []\n",
        "\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            qa_df = process_pdf_q_and_a(file_path, company_df, conference_df)\n",
        "            all_qa_data.append(qa_df)\n",
        "\n",
        "    final_df = pd.concat(all_qa_data, ignore_index=True)\n",
        "\n",
        "    final_df[\"Credential\"] = \"Operator\"\n",
        "\n",
        "    # Apply the lookup function to each row in final_df\n",
        "    final_df['Credential'] = final_df.apply(lambda row: lookup_credential(row, company_df, conference_df), axis=1)\n",
        "\n",
        "    return final_df\n",
        "\n",
        "def lookup_credential(row, company_df, conference_df):\n",
        "    match_designation = company_df.loc[(company_df['Name'] == row['Name']) & (company_df['Year'] == row['Year']) & (company_df['Quarter'] == row['Quarter'])]\n",
        "    match_bank = conference_df.loc[(conference_df['Name'] == row['Name']) & (conference_df['Year'] == row['Year']) & (conference_df['Quarter'] == row['Quarter'])]\n",
        "\n",
        "    if not match_designation.empty:\n",
        "        return match_designation['Designation'].values[0]\n",
        "    elif not match_bank.empty:\n",
        "        return match_bank['Bank'].values[0]\n",
        "    else:\n",
        "        return row['Credential']\n"
      ],
      "metadata": {
        "id": "H1qLniHUPAyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pdf(file_path):\n",
        "    try:\n",
        "        # Extract file and bank names\n",
        "        base_name = os.path.basename(file_path)\n",
        "        bank_name = os.path.basename(os.path.dirname(file_path))  # Assuming bank name is in the parent directory name\n",
        "\n",
        "        # Extract first page text to determine year and quarter\n",
        "        text = extract_first_page_text(file_path)\n",
        "        quarter, year = extract_year_quarter_from_text(text)\n",
        "\n",
        "        # Initialize variable for full text extraction\n",
        "        text = \"\"\n",
        "\n",
        "        # Open and read PDF, handling errors in extraction\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                try:\n",
        "                    text += page.extract_text() + \"\\n\"\n",
        "                except Exception as page_error:\n",
        "                    logging.error(f\"Error extracting text from page in {file_path}: {page_error}\")\n",
        "                    continue  # Skip the problematic page\n",
        "\n",
        "        # Clean the extracted text\n",
        "        cleaned_text = clean_text(text)\n",
        "\n",
        "        # Extract company and conference participants from cleaned text\n",
        "        company_participants, conference_participants = extract_participants_from_text(cleaned_text)\n",
        "\n",
        "        # Return the gathered data\n",
        "        return base_name, year, quarter, bank_name, cleaned_text, company_participants, conference_participants\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing PDF {file_path}: {e}\")\n",
        "        return base_name, None, None, bank_name, \"\", [], []\n",
        "\n",
        "\n",
        "def discover_files(folder_path, sample=None, sample_size=3):\n",
        "    \"\"\"Discovers all PDF files in a folder and its subfolders, optionally sampling them.\"\"\"\n",
        "    logging.info(f\"Starting to discover files in folder: {folder_path}\")\n",
        "    folder_files = defaultdict(list)\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file_name in files:\n",
        "            if file_name.endswith(\".pdf\"):\n",
        "                folder_files[os.path.basename(root)].append(os.path.join(root, file_name))\n",
        "\n",
        "    if sample:\n",
        "        file_paths = [files[:sample_size] for folder, files in folder_files.items()]\n",
        "    else:\n",
        "        file_paths = [file for files in folder_files.values() for file in files]\n",
        "\n",
        "    return file_paths\n",
        "\n",
        "def process_files(file_paths):\n",
        "\n",
        "    \"\"\"Processes PDF files to extract year, quarter, bank, full document text, and company participants.\"\"\"\n",
        "    document_data = []  # To store document-level data\n",
        "    participant_data = []  # To store participant-level data\n",
        "\n",
        "\n",
        "    logging.info(f\"Starting to process {len(file_paths)} PDF files.\")\n",
        "    logging.info(f\"Sample file: {file_paths[0]}\")\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        logging.info(f\"Processing file: {file_path}\")\n",
        "\n",
        "        try:\n",
        "            base_name, year, quarter, bank, text, company_participants, _ = process_pdf(file_path)\n",
        "            if year and quarter:\n",
        "                # Store document-level data\n",
        "                document_data.append({\n",
        "                    \"Year\": year,\n",
        "                    \"Quarter\": quarter,\n",
        "                    \"Text\": text,\n",
        "                    \"Bank\": bank,\n",
        "                    \"File\": base_name\n",
        "                })\n",
        "\n",
        "                # Store participant-level data\n",
        "                for name, designation in company_participants:\n",
        "                    participant_data.append({\n",
        "                        \"Year\": year,\n",
        "                        \"Quarter\": quarter,\n",
        "                        \"Name\": name,\n",
        "                        \"Designation\": designation,\n",
        "                        \"File\": base_name,\n",
        "                        \"Bank\": bank\n",
        "                    })\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "    # Create DataFrames to store document-level and participant-level information\n",
        "    document_df = pd.DataFrame(document_data, columns=[\"Year\", \"Quarter\", \"Text\", \"Bank\", \"File\"])\n",
        "    participant_df = pd.DataFrame(participant_data, columns=[\"Year\", \"Quarter\", \"Name\", \"Designation\", \"File\", \"Bank\"])\n",
        "    logging.info(f\"Successfully processed {len(document_data)} out of {len(file_paths)} PDF files.\")\n",
        "\n",
        "    return document_df, participant_df\n",
        "\n",
        "def save_to_csv(dataframe, save_folder, filename):\n",
        "    \"\"\"Saves the DataFrame to a CSV file.\"\"\"\n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "    csv_path = os.path.join(save_folder, filename)\n",
        "    logging.info(f\"Saving data to CSV at: {csv_path}\")\n",
        "    dataframe.to_csv(csv_path, index=False)\n",
        "    logging.info(\"Data successfully saved to CSV.\")\n",
        "\n",
        "def process_all_documents(raw_folder, processed_folder, metadata_folder, sample=False, sample_size=1):\n",
        "    \"\"\"\n",
        "    Main function to process all PDF files and save both document and participant data.\n",
        "\n",
        "    Args:\n",
        "        raw_folder (str): Path to the folder containing raw PDF files.\n",
        "        processed_folder (str): Path to the folder where processed CSVs will be saved.\n",
        "        sample (bool): Whether to sample files for testing.\n",
        "        sample_size (int): Number of files to sample if `sample=True`.\n",
        "\n",
        "    Returns:\n",
        "        tuple: DataFrames for document-level and participant-level data.\n",
        "    \"\"\"\n",
        "    # Step 1: Discover files, with sampling option\n",
        "    file_paths = discover_files(raw_folder, sample, sample_size)\n",
        "    logging.info(f\"Discovered {len(file_paths)} PDF files.\")\n",
        "\n",
        "    # Step 2: Process files to extract document and participant data\n",
        "    document_df, participant_df = process_files(file_paths)\n",
        "\n",
        "    # Step 3: Save to CSV\n",
        "    save_to_csv(document_df, save_folder=processed_folder, filename=\"pdf_summarytext_data.csv\")\n",
        "    save_to_csv(participant_df, save_folder=metadata_folder, filename=\"company_participants.csv\")\n",
        "\n",
        "    # Return DataFrames\n",
        "    return document_df, participant_df"
      ],
      "metadata": {
        "id": "631wDbAyPOWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    raw_folder = \"\"  # Path to raw PDF files\n",
        "    processed_folder = \"\"  # Path to save processed CSVs\n",
        "    metadata_folder = \"\"  # Path to save metadata CSV\n",
        "\n",
        "    logging.info(\"Processing begins for sentiment analysis.\")\n",
        "    company_df, conference_df, document_df = process_folder(f'{raw_folder}/{bank}')\n",
        "    final_qa_df = process_folder_q_and_a(f'{raw_folder}/{bank}', company_df, conference_df)\n",
        "\n",
        "    save_to_csv(company_df, save_folder=f'{processed_folder}/{bank}', filename='company_df.csv')\n",
        "    save_to_csv(conference_df, save_folder=f'{processed_folder}/{bank}', filename='conference_df.csv')\n",
        "    save_to_csv(final_qa_df, save_folder=f'{processed_folder}/{bank}', filename='final_qa_df.csv')\n",
        "\n",
        "    logging.info(\"Processing complete for sentiment analysis.\")\n",
        "\n",
        "    logging.info(\"Processing begins for metadata\")\n",
        "\n",
        "    # Run the document processing function\n",
        "    document_df, participant_df = process_all_documents(raw_folder, processed_folder, metadata_folder, sample=False)\n",
        "    logging.info(\"Processing complete for metadata\")\n",
        "\n",
        "    logging.info(\"Processing complete.\")"
      ],
      "metadata": {
        "id": "IYf1TowcPoqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transcript Summarisation with NLTK"
      ],
      "metadata": {
        "id": "HBwKKzsxQG3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "du3y-a9EmxQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = punctuation + '\\n' + '—' + '“' + ',' + '”' + '‘' + '-' + '’'\n",
        "\n",
        "contractions_dict = {\n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"doesn’t\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"don’t\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so is\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y’all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\",\n",
        "\"ain’t\": \"am not\",\n",
        "\"aren’t\": \"are not\",\n",
        "\"can’t\": \"cannot\",\n",
        "\"can’t’ve\": \"cannot have\",\n",
        "\"’cause\": \"because\",\n",
        "\"could’ve\": \"could have\",\n",
        "\"couldn’t\": \"could not\",\n",
        "\"couldn’t’ve\": \"could not have\",\n",
        "\"didn’t\": \"did not\",\n",
        "\"doesn’t\": \"does not\",\n",
        "\"don’t\": \"do not\",\n",
        "\"don’t\": \"do not\",\n",
        "\"hadn’t\": \"had not\",\n",
        "\"hadn’t’ve\": \"had not have\",\n",
        "\"hasn’t\": \"has not\",\n",
        "\"haven’t\": \"have not\",\n",
        "\"he’d\": \"he had\",\n",
        "\"he’d’ve\": \"he would have\",\n",
        "\"he’ll\": \"he will\",\n",
        "\"he’ll’ve\": \"he will have\",\n",
        "\"he’s\": \"he is\",\n",
        "\"how’d\": \"how did\",\n",
        "\"how’d’y\": \"how do you\",\n",
        "\"how’ll\": \"how will\",\n",
        "\"how’s\": \"how is\",\n",
        "\"i’d\": \"i would\",\n",
        "\"i’d’ve\": \"i would have\",\n",
        "\"i’ll\": \"i will\",\n",
        "\"i’ll’ve\": \"i will have\",\n",
        "\"i’m\": \"i am\",\n",
        "\"i’ve\": \"i have\",\n",
        "\"isn’t\": \"is not\",\n",
        "\"it’d\": \"it would\",\n",
        "\"it’d’ve\": \"it would have\",\n",
        "\"it’ll\": \"it will\",\n",
        "\"it’ll’ve\": \"it will have\",\n",
        "\"it’s\": \"it is\",\n",
        "\"let’s\": \"let us\",\n",
        "\"ma’am\": \"madam\",\n",
        "\"mayn’t\": \"may not\",\n",
        "\"might’ve\": \"might have\",\n",
        "\"mightn’t\": \"might not\",\n",
        "\"mightn’t’ve\": \"might not have\",\n",
        "\"must’ve\": \"must have\",\n",
        "\"mustn’t\": \"must not\",\n",
        "\"mustn’t’ve\": \"must not have\",\n",
        "\"needn’t\": \"need not\",\n",
        "\"needn’t’ve\": \"need not have\",\n",
        "\"o’clock\": \"of the clock\",\n",
        "\"oughtn’t\": \"ought not\",\n",
        "\"oughtn’t’ve\": \"ought not have\",\n",
        "\"shan’t\": \"shall not\",\n",
        "\"sha’n’t\": \"shall not\",\n",
        "\"shan’t’ve\": \"shall not have\",\n",
        "\"she’d\": \"she would\",\n",
        "\"she’d’ve\": \"she would have\",\n",
        "\"she’ll\": \"she will\",\n",
        "\"she’ll’ve\": \"she will have\",\n",
        "\"she’s\": \"she is\",\n",
        "\"should’ve\": \"should have\",\n",
        "\"shouldn’t\": \"should not\",\n",
        "\"shouldn’t’ve\": \"should not have\",\n",
        "\"so’ve\": \"so have\",\n",
        "\"so’s\": \"so is\",\n",
        "\"that’d\": \"that would\",\n",
        "\"that’d’ve\": \"that would have\",\n",
        "\"that’s\": \"that is\",\n",
        "\"there’d\": \"there would\",\n",
        "\"there’d’ve\": \"there would have\",\n",
        "\"there’s\": \"there is\",\n",
        "\"they’d\": \"they would\",\n",
        "\"they’d’ve\": \"they would have\",\n",
        "\"they’ll\": \"they will\",\n",
        "\"they’ll’ve\": \"they will have\",\n",
        "\"they’re\": \"they are\",\n",
        "\"they’ve\": \"they have\",\n",
        "\"to’ve\": \"to have\",\n",
        "\"wasn’t\": \"was not\",\n",
        "\"we’d\": \"we would\",\n",
        "\"we’d’ve\": \"we would have\",\n",
        "\"we’ll\": \"we will\",\n",
        "\"we’ll’ve\": \"we will have\",\n",
        "\"we’re\": \"we are\",\n",
        "\"we’ve\": \"we have\",\n",
        "\"weren’t\": \"were not\",\n",
        "\"what’ll\": \"what will\",\n",
        "\"what’ll’ve\": \"what will have\",\n",
        "\"what’re\": \"what are\",\n",
        "\"what’s\": \"what is\",\n",
        "\"what’ve\": \"what have\",\n",
        "\"when’s\": \"when is\",\n",
        "\"when’ve\": \"when have\",\n",
        "\"where’d\": \"where did\",\n",
        "\"where’s\": \"where is\",\n",
        "\"where’ve\": \"where have\",\n",
        "\"who’ll\": \"who will\",\n",
        "\"who’ll’ve\": \"who will have\",\n",
        "\"who’s\": \"who is\",\n",
        "\"who’ve\": \"who have\",\n",
        "\"why’s\": \"why is\",\n",
        "\"why’ve\": \"why have\",\n",
        "\"will’ve\": \"will have\",\n",
        "\"won’t\": \"will not\",\n",
        "\"won’t’ve\": \"will not have\",\n",
        "\"would’ve\": \"would have\",\n",
        "\"wouldn’t\": \"would not\",\n",
        "\"wouldn’t’ve\": \"would not have\",\n",
        "\"y’all\": \"you all\",\n",
        "\"y’all\": \"you all\",\n",
        "\"y’all’d\": \"you all would\",\n",
        "\"y’all’d’ve\": \"you all would have\",\n",
        "\"y’all’re\": \"you all are\",\n",
        "\"y’all’ve\": \"you all have\",\n",
        "\"you’d\": \"you would\",\n",
        "\"you’d’ve\": \"you would have\",\n",
        "\"you’ll\": \"you will\",\n",
        "\"you’ll’ve\": \"you will have\",\n",
        "\"you’re\": \"you are\",\n",
        "\"you’re\": \"you are\",\n",
        "\"you’ve\": \"you have\",\n",
        "}\n",
        "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "# Function to clean the html from the article\n",
        "def cleanhtml(raw_html):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\n",
        "    return cleantext\n",
        "\n",
        "# Function expand the contractions if there's any\n",
        "def expand_contractions(s, contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, s)\n",
        "\n",
        "# Function which generates the summary of the articles (This uses the 20% of the sentences with the highest score)\n",
        "def summary(sentence_score_OwO):\n",
        "    summary_list = []\n",
        "    for summ in sentence_score_OwO:\n",
        "        select_length = int(len(summ)*0.25)\n",
        "        summary_ = nlargest(select_length, summ, key = summ.get)\n",
        "        summary_list.append(\".\".join(summary_))\n",
        "    return summary_list\n",
        "\n",
        "# Function to normalize the word frequency which is used in the function word_frequency\n",
        "def normalize(li_word):\n",
        "    global normalized_freq\n",
        "    normalized_freq = []\n",
        "    for dictionary in li_word:\n",
        "        max_frequency = max(dictionary.values())\n",
        "        for word in dictionary.keys():\n",
        "            dictionary[word] = dictionary[word]/max_frequency\n",
        "        normalized_freq.append(dictionary)\n",
        "    return normalized_freq\n",
        "\n",
        "# Function to calculate the word frequency\n",
        "def word_frequency(article_word):\n",
        "    word_frequency = {}\n",
        "    li_word = []\n",
        "    for sentence in article_word:\n",
        "        for word in word_tokenize(sentence):\n",
        "            if word not in word_frequency.keys():\n",
        "                word_frequency[word] = 1\n",
        "            else:\n",
        "                word_frequency[word] += 1\n",
        "        li_word.append(word_frequency)\n",
        "        word_frequency = {}\n",
        "    normalize(li_word)\n",
        "    return normalized_freq\n",
        "\n",
        "# Function to Score the sentence which is called in the function sent_token\n",
        "def sentence_score(li):\n",
        "    global sentence_score_list\n",
        "    sentence_score = {}\n",
        "    sentence_score_list = []\n",
        "    for list_, dictionary in zip(li, normalized_freq):\n",
        "        for sent in list_:\n",
        "            for word in word_tokenize(sent):\n",
        "                if word in dictionary.keys():\n",
        "                    if sent not in sentence_score.keys():\n",
        "                        sentence_score[sent] = dictionary[word]\n",
        "                    else:\n",
        "                        sentence_score[sent] += dictionary[word]\n",
        "        sentence_score_list.append(sentence_score)\n",
        "        sentence_score = {}\n",
        "    return sentence_score_list\n",
        "\n",
        "# Function to tokenize the sentence\n",
        "def sent_token(article_sent):\n",
        "    sentence_list = []\n",
        "    sent_token = []\n",
        "    for sent in article_sent:\n",
        "        token = sent_tokenize(sent)\n",
        "        for sentence in token:\n",
        "            token_2 = ''.join(word for word in sentence if word not in punctuation)\n",
        "            token_2 = re.sub(' +', ' ',token_2)\n",
        "            sent_token.append(token_2)\n",
        "        sentence_list.append(sent_token)\n",
        "        sent_token = []\n",
        "    sentence_score(sentence_list)\n",
        "    return sentence_score_list\n",
        "\n",
        "# Function to preprocess the articles\n",
        "def preprocessing(article):\n",
        "    global article_sent\n",
        "\n",
        "    # Converting to lowercase\n",
        "    article = article.str.lower()\n",
        "\n",
        "    # Removing the HTML\n",
        "    article = article.apply(lambda x: cleanhtml(x))\n",
        "\n",
        "    # Removing the email ids\n",
        "    article = article.apply(lambda x: re.sub('\\S+@\\S+','', x))\n",
        "\n",
        "    # Removing The URLS\n",
        "    article = article.apply(lambda x: re.sub(\"((http\\://|https\\://|ftp\\://)|(www.))+(([a-zA-Z0-9\\.-]+\\.[a-zA-Z]{2,4})|([0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}))(/[a-zA-Z0-9%:/-_\\?\\.'~]*)?\",'', x))\n",
        "\n",
        "    # Removing the '\\xa0'\n",
        "    article = article.apply(lambda x: x.replace(\"\\xa0\", \" \"))\n",
        "\n",
        "    # Removing the contractions\n",
        "    article = article.apply(lambda x: expand_contractions(x))\n",
        "\n",
        "    # Stripping the possessives\n",
        "    article = article.apply(lambda x: x.replace(\"'s\", ''))\n",
        "    article = article.apply(lambda x: x.replace('’s', ''))\n",
        "    article = article.apply(lambda x: x.replace(\"\\'s\", ''))\n",
        "    article = article.apply(lambda x: x.replace(\"\\’s\", ''))\n",
        "\n",
        "    # Removing the Trailing and leading whitespace and double spaces\n",
        "    article = article.apply(lambda x: re.sub(' +', ' ',x))\n",
        "\n",
        "    # Copying the article for the sentence tokenization\n",
        "    article_sent = article.copy()\n",
        "\n",
        "    # Removing punctuations from the article\n",
        "    article = article.apply(lambda x: ''.join(word for word in x if word not in punctuation))\n",
        "\n",
        "    # Removing the Trailing and leading whitespace and double spaces again as removing punctuation might\n",
        "    # Lead to a white space\n",
        "    article = article.apply(lambda x: re.sub(' +', ' ',x))\n",
        "\n",
        "    # Removing the Stopwords\n",
        "    article = article.apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
        "\n",
        "    return article\n",
        "\n",
        "# Functions to change the article string (if passed) to change it to generate a pandas series\n",
        "def make_series(art):\n",
        "    global dataframe\n",
        "    data_dict = {'article' : [art]}\n",
        "    dataframe = pd.DataFrame(data_dict)['article']\n",
        "    return dataframe\n",
        "\n",
        "# Function which is to be called to generate the summary which in further calls other functions alltogether\n",
        "def article_summarize(artefact):\n",
        "\n",
        "    if type(artefact) != pd.Series:\n",
        "        artefact = make_series(artefact)\n",
        "\n",
        "    df = preprocessing(artefact)\n",
        "\n",
        "    word_normalization = word_frequency(df)\n",
        "\n",
        "    sentence_score_OwO = sent_token(article_sent)\n",
        "\n",
        "    summarized_article = summary(sentence_score_OwO)\n",
        "\n",
        "    return summarized_article"
      ],
      "metadata": {
        "id": "t2yNB-OtP_Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  processed_folder = \"\"  # Path to save processed CSVs\n",
        "\n",
        "  csv_path = f'{processed_folder}/{bank}/final_qa_df.csv'\n",
        "  transcript_df = pd.read_csv(csv_path)\n",
        "\n",
        "  summaries = article_summarize(transcript_df['Dialogue'])\n",
        "\n",
        "  transcript_df[\"Summarised_dialogue\"] = \"\"\n",
        "  for i, row in transcript_df.iterrows():\n",
        "      transcript_df.loc[transcript_df.index == i, 'Summarised_dialogue'] = summaries[i]\n",
        "\n",
        "  save_to_csv(transcript_df, save_folder=f'{processed_folder}/{bank}', filename='final_qa_df.csv')"
      ],
      "metadata": {
        "id": "WSZVOgVfRCfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis - using yiyanghkust/finbert-tone"
      ],
      "metadata": {
        "id": "zF5cT4aIRZ9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "GFKDbS7RRjYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface langchain chromadb pypdf sentence-transformers accelerate langchain-community"
      ],
      "metadata": {
        "id": "vELhKvTRRvGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "OcXUS7LtR99u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
        "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
        "\n",
        "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "iC-pEAGxSBNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_result(results):\n",
        "  positive_count = 0\n",
        "  neutral_count = 0\n",
        "  negative_count = 0\n",
        "\n",
        "  for result in results:\n",
        "    if result[\"label\"] == \"Positive\":\n",
        "      positive_count += 1\n",
        "    elif result[\"label\"] == \"Neutral\":\n",
        "      neutral_count += 1\n",
        "    else:\n",
        "      negative_count += 1\n",
        "\n",
        "  total_count = len(results)\n",
        "  negative_pct = (negative_count*100)/total_count\n",
        "  positive_pct = (positive_count*100)/total_count\n",
        "  neutral_pct = (neutral_count*100)/total_count\n",
        "\n",
        "  sentiment_pct = {\n",
        "    \"Positive\": positive_pct,\n",
        "    \"Neutral\": neutral_pct,\n",
        "    \"Negative\": negative_pct,\n",
        "  }\n",
        "\n",
        "  # Get the dominant sentiment\n",
        "  dominant_sentiment = max(sentiment_pct, key=sentiment_pct.get)\n",
        "\n",
        "  return dominant_sentiment, sentiment_pct\n",
        "\n",
        "# Initialize the RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "   chunk_size=512,\n",
        "   chunk_overlap=20,\n",
        "   length_function=len,\n",
        "   add_start_index=True,\n",
        ")\n",
        "\n",
        "def analyse_detail_sentiment(transcript_df, bank, dialogue_col):\n",
        "  detail_sentiment_data = []\n",
        "  transcript_grouped_df = transcript_df.groupby(by=[\"Year\", \"Quarter\"])\n",
        "  for name, groups in transcript_grouped_df:\n",
        "      for i, row in groups.iterrows():\n",
        "        text = row[dialogue_col]\n",
        "        if text:\n",
        "          #sentences = [text[i:i+512] for i in range(0, len(text), 512)]\n",
        "          documents = [Document(\n",
        "            page_content=text,\n",
        "            metadata=row.to_dict()\n",
        "          )]\n",
        "          chunks = text_splitter.split_documents(documents)\n",
        "          sentences = [chunk.page_content for chunk in chunks]\n",
        "          results = nlp(sentences)\n",
        "\n",
        "          sentiment, scores = get_result(results)\n",
        "\n",
        "          detail_sentiment_data.append({\n",
        "              \"Year\": row['Year'],\n",
        "              \"Quarter\": row['Quarter'],\n",
        "              \"Sentiment\": sentiment,\n",
        "              \"Sentiment_Score\": round(scores[sentiment], 2),\n",
        "              \"Bank\": bank,\n",
        "              \"Section\": row['Section']\n",
        "          })\n",
        "\n",
        "  return pd.DataFrame(detail_sentiment_data)"
      ],
      "metadata": {
        "id": "O_JG8vFWSDJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment_percentage(groups, total_count):\n",
        "    negative_count = len(groups[(groups[\"Sentiment\"]==\"Negative\")])\n",
        "    negative_pct = (negative_count*100)/total_count\n",
        "\n",
        "    positive_count = len(groups[(groups[\"Sentiment\"]==\"Positive\")])\n",
        "    positive_pct = (positive_count*100)/total_count\n",
        "\n",
        "    neutral_count = len(groups[(groups[\"Sentiment\"]==\"Neutral\")])\n",
        "    neutral_pct = (neutral_count*100)/total_count\n",
        "\n",
        "    sentiment_pct = {\n",
        "      \"Positive\": positive_pct,\n",
        "      \"Neutral\": neutral_pct,\n",
        "      \"Negative\": negative_pct,\n",
        "    }\n",
        "\n",
        "    # Get the dominant sentiment\n",
        "    dominant_sentiment = max(sentiment_pct, key=sentiment_pct.get)\n",
        "\n",
        "    return dominant_sentiment, sentiment_pct\n",
        "\n",
        "def summarise_sentiments(sentiment_df, section, dialogue_col):\n",
        "    sentiment_detail_grouped_df = sentiment_df.groupby(by=[\"Year\", \"Quarter\"])\n",
        "\n",
        "    quaterly_sentiment_data = []\n",
        "    for name, groups in sentiment_detail_grouped_df:\n",
        "        for i, row in groups.iterrows():\n",
        "            year = row[\"Year\"]\n",
        "            quarter = row[\"Quarter\"]\n",
        "\n",
        "        total_count = groups.Bank.count()\n",
        "\n",
        "        sentiment, sentiment_pct = get_sentiment_percentage(groups, total_count)\n",
        "\n",
        "        quaterly_sentiment_data.append({\n",
        "            \"Year\": row['Year'],\n",
        "            \"Quarter\": row['Quarter'],\n",
        "            \"Bank\": row['Bank'],\n",
        "            \"Section\": section,\n",
        "            \"Sentiment\": sentiment,\n",
        "            \"Positivity\": round(sentiment_pct[\"Positive\"], 2),\n",
        "            \"Neutrality\": round(sentiment_pct[\"Neutral\"], 2),\n",
        "            \"Negativity\": round(sentiment_pct[\"Negative\"], 2)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(quaterly_sentiment_data)\n",
        "\n",
        "def get_combined_sentiment_pct(row, q_qa_sentiment_df, q_presentation_sentiment_df, sentiment_col):\n",
        "    qa_positivity = q_qa_sentiment_df.loc[(q_qa_sentiment_df['Year'] == row['Year']) & (q_qa_sentiment_df['Quarter'] == row['Quarter'])][sentiment_col]\n",
        "    presentation_positivity = q_presentation_sentiment_df.loc[(q_presentation_sentiment_df['Year'] == row['Year']) & (q_presentation_sentiment_df['Quarter'] == row['Quarter'])][sentiment_col]\n",
        "    combined_positivity = presentation_positivity.values[0] + qa_positivity.values[0]\n",
        "    if combined_positivity > 0:\n",
        "        combined_positivity = combined_positivity / 2\n",
        "    return combined_positivity\n",
        "\n",
        "def get_combined_sentiment(q_qa_sentiment_df, q_presentation_sentiment_df):\n",
        "    quaterly_sentiment_data = []\n",
        "    for i, row in q_presentation_sentiment_df.iterrows():\n",
        "        combined_positivity = get_combined_sentiment_pct(row, q_qa_sentiment_df, q_presentation_sentiment_df, \"Positivity\")\n",
        "        combined_neutrality = get_combined_sentiment_pct(row, q_qa_sentiment_df, q_presentation_sentiment_df, \"Neutrality\")\n",
        "        combined_negativity = get_combined_sentiment_pct(row, q_qa_sentiment_df, q_presentation_sentiment_df, \"Negativity\")\n",
        "\n",
        "        if combined_negativity >= 7.5:\n",
        "            sentiment = \"Negative\"\n",
        "        elif combined_positivity > combined_neutrality:\n",
        "            sentiment = \"Positive\"\n",
        "        else:\n",
        "            sentiment = \"Neutral\"\n",
        "\n",
        "        quaterly_sentiment_data.append({\n",
        "            \"Year\": row['Year'],\n",
        "            \"Quarter\": row['Quarter'],\n",
        "            \"Bank\": row['Bank'],\n",
        "            \"Section\": 'Combined',\n",
        "            \"Sentiment\": sentiment,\n",
        "            \"Positivity\": round(combined_positivity, 2),\n",
        "            \"Neutrality\": round(combined_neutrality, 2),\n",
        "            \"Negativity\": round(combined_negativity, 2)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(quaterly_sentiment_data)"
      ],
      "metadata": {
        "id": "KD5xdOHZSGkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  processed_folder = \"\"  # Path to save processed CSVs\n",
        "\n",
        "  csv_path = f'{processed_folder}/{bank}/final_qa_df.csv'\n",
        "  transcript_df = pd.read_csv(csv_path)\n",
        "  transcript_df = transcript_df[transcript_df['Text Type'] != \"Operator\"]\n",
        "\n",
        "  dialogue_cols = [\"Summarised_dialogue\", \"Dialogue\"]\n",
        "  for dialogue_col in dialogue_cols:\n",
        "\n",
        "      dialog_df = transcript_df.dropna(subset=[dialogue_col])\n",
        "\n",
        "      # Sentiment analysis for dialog\n",
        "      detail_sentiment_df = analyse_detail_sentiment(dialog_df, bank, dialogue_col)\n",
        "      save_to_csv(detail_sentiment_df, save_folder=f'{processed_folder}/{bank}', filename=f'detail_sentiment_df_{dialogue_col}.csv')\n",
        "\n",
        "      # Summarise quaterly sentiments - Presentation section\n",
        "      presentation_sentiment_df = detail_sentiment_df[detail_sentiment_df['Section'] == \"Presentation\"]\n",
        "      quaterly_presentation_sentiment_df = summarise_sentiments(presentation_sentiment_df, \"Presentation\", dialogue_col)\n",
        "      save_to_csv(quaterly_presentation_sentiment_df, save_folder=f'{processed_folder}/{bank}', filename=f'quaterly_presentation_sentiment_df_{dialogue_col}.csv')\n",
        "\n",
        "      # Summarise quaterly sentiments - Question-and-Answer section\n",
        "      qa_sentiment_df = detail_sentiment_df[detail_sentiment_df['Section'] == \"Question-and-Answer\"]\n",
        "      quaterly_qa_sentiment_df = summarise_sentiments(qa_sentiment_df, \"Question-and-Answer\", dialogue_col)\n",
        "      save_to_csv(quaterly_qa_sentiment_df, save_folder=f'{processed_folder}/{bank}', filename=f'quaterly_qa_sentiment_df_{dialogue_col}.csv')\n",
        "\n",
        "      # Summarise quaterly sentiments - combined\n",
        "      quaterly_combined_sentiment_df = get_combined_sentiment(quaterly_qa_sentiment_df, quaterly_presentation_sentiment_df)\n",
        "      save_to_csv(quaterly_combined_sentiment_df, save_folder=f'{processed_folder}/{bank}', filename=f'quaterly_combined_sentiment_df_{dialogue_col}.csv')"
      ],
      "metadata": {
        "id": "bwopFD3KSPuX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}