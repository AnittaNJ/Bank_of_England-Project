{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK - WordCloud"
      ],
      "metadata": {
        "id": "5IX5rmr_SnMn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw9_5aAztOpQ"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import string\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"all\")"
      ],
      "metadata": {
        "id": "yts5VIWo8Zo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define stop words and punctuation.\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# construct list of common words to exclude that are not covered by stopwords\n",
        "excludes = [\"one\", \"two\", \"three\", \"four\", \"five\", \"first\", \"second\", \"third\", \"fourth\", \"year\", \"twenty\", \"quarter\", \"thousand\", \"think\",\n",
        "            \"question\", \"see\", \"also\", \"would\", \"thank\", \"you\", \"u\", \"chf\", \"yes\", \"sure\", \"just\", \"is\", \"morning\", \"begin\", \"actually\", \"clearly\",\n",
        "            \"said\", \"look\", \"say\", \"obviously\", \"really\", \"credit\", \"suisse\", \"thomas\", \"group\", \"david\", \"c\"]\n",
        "\n",
        "# instantiate WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "u4AUAzpMurQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean the document and display word cloud.\n",
        "def word_cloud_display(df, key):\n",
        "  top_number_of_results = 50\n",
        "  all_transcripts = df[key].str.lower().str.cat(sep=' ')\n",
        "  all_text = re.sub('[^A-Za-z]+', ' ', all_transcripts)\n",
        "\n",
        "  word_tokens = word_tokenize(all_text)\n",
        "\n",
        "  filtered_all_transcripts = [lemmatizer.lemmatize(word) for word in word_tokens if (word not in stop_words) and not(word.isnumeric()) and (word not in excludes)]\n",
        "\n",
        "  word_distribution = nltk.FreqDist(filtered_all_transcripts)\n",
        "  transcript_word_frequency_distribution_df = pd.DataFrame(word_distribution.most_common(top_number_of_results), columns=['Word', 'Frequency'])\n",
        "\n",
        "  plt.figure(figsize=(8,8))\n",
        "  sns.set_style(\"whitegrid\")\n",
        "  ax = sns.barplot(x = \"Word\", y = \"Frequency\", data = transcript_word_frequency_distribution_df.head(10))\n",
        "\n",
        "  plt.figure(figsize = (60,60))\n",
        "  wc = WordCloud(background_color = 'black', max_words = 1000,  max_font_size = 50)\n",
        "  wc.generate(' '.join(filtered_all_transcripts))\n",
        "  plt.imshow(wc)\n",
        "  plt.axis('off')"
      ],
      "metadata": {
        "id": "18I-oeQBtmvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bank = ''\n",
        "\n",
        "# load the transcript file\n",
        "def load_transcript():\n",
        "  # get the transcript\n",
        "  csv_path = f'/final_qa_df.csv'\n",
        "\n",
        "  return pd.read_csv(csv_path)\n",
        "\n",
        "transcript_df = load_transcript()"
      ],
      "metadata": {
        "id": "UYwC_sZSwr3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordCloud - Quarterly - Original Transcript"
      ],
      "metadata": {
        "id": "ewvuh7ESEY2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word cloud for each quarter - Original transcript\n",
        "transcript_grouped_df = transcript_df.groupby(by=[\"Year\", \"Quarter\"])\n",
        "\n",
        "for name, groups in transcript_grouped_df:\n",
        "  word_cloud_display(groups, \"Dialogue\")"
      ],
      "metadata": {
        "id": "tZzDqQ60t58Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordCloud - Quarterly - Summarised Transcript"
      ],
      "metadata": {
        "id": "3A1diOfwEtZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word cloud for each quarter - Summarised transcript\n",
        "\n",
        "for name, groups in transcript_grouped_df:\n",
        "  word_cloud_display(groups, \"Summarised_dialogue\")"
      ],
      "metadata": {
        "id": "K86wDGKEDov6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERTopic"
      ],
      "metadata": {
        "id": "3UX3-rEuS9XW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install BERTopic\n",
        "!pip install bertopic"
      ],
      "metadata": {
        "id": "04dFACGi4phL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pipeline and BERTopic\n",
        "from transformers import pipeline\n",
        "from bertopic import BERTopic"
      ],
      "metadata": {
        "id": "fNsdn_ij400h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define function to clean transcript text\n",
        "def clean_transcript_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
        "\n",
        "    word_tokens = word_tokenize(text)\n",
        "\n",
        "    for word in word_tokens:\n",
        "      if ((word in stop_words) or word.isnumeric() or (word in excludes)):\n",
        "        text = text.replace(\" \" + word + \" \", \" \")\n",
        "      else:\n",
        "        word_lemma = lemmatizer.lemmatize(word)\n",
        "        text = text.replace(word, word_lemma)\n",
        "    return text\n",
        "\n",
        "#define function to clean transcript text\n",
        "def clean_transcripts(transcripts_list):\n",
        "  transcripts_list_clean = []\n",
        "  for transcript in transcripts_list:\n",
        "    if transcript is not None and transcript != \"nan\" and isinstance(transcript, str):\n",
        "      transcript =  clean_transcript_text(transcript)\n",
        "      transcripts_list_clean.append(transcript)\n",
        "\n",
        "  return transcripts_list_clean"
      ],
      "metadata": {
        "id": "L_DZoDRB5GE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate BERTopic model and fit with transcript\n",
        "def get_bertopic_model(section, dialogue_col=\"Dialogue\"):\n",
        "  transcript_answers_df = transcript_df.loc[(transcript_df['Text Type'] == \"Answer\")]\n",
        "  if section:\n",
        "    transcript_answers_df = transcript_answers_df.loc[(transcript_answers_df['Section'] == section)]\n",
        "  transcript_answers_list = transcript_answers_df[dialogue_col].tolist()\n",
        "  transcript_answers_list_clean = clean_transcripts(transcript_answers_list)\n",
        "\n",
        "  model = BERTopic(verbose=False)\n",
        "  topic, probabilities = model.fit_transform(transcript_answers_list_clean)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "pKObDpjN8Vmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original Transcript"
      ],
      "metadata": {
        "id": "MFfYzgKpRiyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic - Combined Sections"
      ],
      "metadata": {
        "id": "ZRv_Bck9INZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\" (empty) for combined dialogues\n",
        "model = get_bertopic_model(\"\")"
      ],
      "metadata": {
        "id": "NC621o139xlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diplay top frequency topics\n",
        "model.get_topic_freq().head(10)"
      ],
      "metadata": {
        "id": "Xg1tpbfu960k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_topic(0)"
      ],
      "metadata": {
        "id": "dFTvXQWX-hPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_topic(1)"
      ],
      "metadata": {
        "id": "EQtEFS0V-jxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_barchart()"
      ],
      "metadata": {
        "id": "viylHt6a-wxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic - Presentation"
      ],
      "metadata": {
        "id": "jo081hEDbEZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_bertopic_model(\"Presentation\")"
      ],
      "metadata": {
        "id": "TE9y04D5bRpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diplay top frequency topics\n",
        "model.get_topic_freq().head(10)"
      ],
      "metadata": {
        "id": "J2vMEUs1fbDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_barchart()"
      ],
      "metadata": {
        "id": "6CcUmbHefesF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic - Question-and-Answer"
      ],
      "metadata": {
        "id": "VEzt_KXQf98z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_bertopic_model(\"Question-and-Answer\")"
      ],
      "metadata": {
        "id": "pRxRiek9fkvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diplay top frequency topics\n",
        "model.get_topic_freq().head(10)"
      ],
      "metadata": {
        "id": "EJWgkiLggFXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_barchart()"
      ],
      "metadata": {
        "id": "UwyTPy-8gITP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarised Transcript"
      ],
      "metadata": {
        "id": "PuWqQeuOaGYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic - Combined Sections"
      ],
      "metadata": {
        "id": "Vo2fKn6xaKY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\" (empty) for combined dialogues\n",
        "model = get_bertopic_model(\"\", \"Summarised_dialogue\")"
      ],
      "metadata": {
        "id": "e_hqF2FlaU0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diplay top frequency topics\n",
        "model.get_topic_freq().head(10)"
      ],
      "metadata": {
        "id": "IHoo9MNsaepP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_barchart()"
      ],
      "metadata": {
        "id": "nPx6Ufq1agpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic - Presentation"
      ],
      "metadata": {
        "id": "VNAZal-Yak9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_bertopic_model(\"Presentation\", \"Summarised_dialogue\")"
      ],
      "metadata": {
        "id": "22hWAYoecf2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diplay top frequency topics\n",
        "model.get_topic_freq().head(10)"
      ],
      "metadata": {
        "id": "Dqw9YHYSclQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_barchart()"
      ],
      "metadata": {
        "id": "UyFvZ9ZVmdcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic - Question-and-Answer"
      ],
      "metadata": {
        "id": "xe-e9r9XmtsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_bertopic_model(\"Question-and-Answer\", \"Summarised_dialogue\")"
      ],
      "metadata": {
        "id": "w_g_GfjSm1i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diplay top frequency topics\n",
        "model.get_topic_freq().head(10)"
      ],
      "metadata": {
        "id": "iyVkcmrym-Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_barchart()"
      ],
      "metadata": {
        "id": "K7yzqgeanB2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gensim - LDA"
      ],
      "metadata": {
        "id": "WTUNId0REq54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim nltk datasets pyLDAvis ipykernel"
      ],
      "metadata": {
        "id": "ylRE3McJJ-mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import Phrases\n",
        "from gensim.models.phrases import Phraser\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string"
      ],
      "metadata": {
        "id": "3-hlM1kV5I66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "Kzg_OXGYAoKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess/clean the text\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    # Remove special characters, digits, and extra whitespace\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # Remove non-alphabetic characters\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize whitespace\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation and word not in excludes]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "ZXnHvsnZ5d82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load transcript from the csv file\n",
        "transcript_df = load_transcript()\n",
        "\n",
        "# get the appropriate section of the transcript\n",
        "def get_transcripts(section):\n",
        "  transcript_answers_df = transcript_df.loc[(transcript_df['Text Type'] == \"Answer\")]\n",
        "  if section:\n",
        "    transcript_answers_df = transcript_answers_df.loc[(transcript_answers_df['Section'] == section)]\n",
        "\n",
        "  return transcript_answers_df\n",
        "\n",
        "# Analyse each quarter\n",
        "def get_topics_per_quarter(transcript_answers_df, dialogue_col=\"Dialogue\"):\n",
        "\n",
        "  transcript_answers_grouped_df = transcript_answers_df.groupby(by=[\"Year\", \"Quarter\"])\n",
        "\n",
        "  for name, groups in transcript_answers_grouped_df:\n",
        "    lda_model = None\n",
        "    documents = groups[\"Dialogue\"].tolist()\n",
        "    if len(documents) > 0:\n",
        "      # Apply preprocessing\n",
        "      processed_docs = [preprocess(doc) for doc in documents]\n",
        "\n",
        "      # Create a dictionary representation of the documents\n",
        "      dictionary = Dictionary(processed_docs)\n",
        "\n",
        "      # Create a bag-of-words corpus\n",
        "      corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "      #lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=4, passes=45, random_state=24)\n",
        "      lda_model = LdaModel(\n",
        "            corpus=corpus,\n",
        "            id2word=dictionary,\n",
        "            chunksize=2000,\n",
        "            alpha='auto',\n",
        "            eta='auto',\n",
        "            iterations=400,\n",
        "            num_topics=5,\n",
        "            passes=30,\n",
        "            eval_every=None\n",
        "          )\n",
        "\n",
        "      # Calculate the coherence score\n",
        "      coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
        "      coherence_score = coherence_model_lda.get_coherence()\n",
        "\n",
        "      print(f'Year: {groups[\"Year\"].tolist()[0]}, Quarter: {groups[\"Quarter\"].tolist()[0]}, Coherence Score: {coherence_score}, Perplexity: {lda_model.log_perplexity(corpus)}')\n",
        "\n",
        "      for idx, topic in lda_model.print_topics(-1):\n",
        "        print(f\"Topic #{idx + 1}: {topic}\")"
      ],
      "metadata": {
        "id": "otRx6zykGC7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original Transcript"
      ],
      "metadata": {
        "id": "iUoc3_c8Vt1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gensim - LDA - Combined Sections"
      ],
      "metadata": {
        "id": "1o4gXRUG-Uqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\" (empty) for combined transcripts\n",
        "transcript_answers_df = get_transcripts(\"\")\n",
        "\n",
        "documents = transcript_answers_df[\"Dialogue\"].tolist()\n",
        "\n",
        "# Apply preprocessing\n",
        "processed_docs = [preprocess(doc) for doc in documents]"
      ],
      "metadata": {
        "id": "FuxNxcr46VUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect bigrams in the corpus\n",
        "bigram = Phrases(processed_docs, min_count=2, threshold=10)  # Adjust min_count and threshold as needed\n",
        "bigram_mod = Phraser(bigram)\n",
        "\n",
        "# Apply bigram model to documents\n",
        "bigram_docs = [bigram_mod[doc] for doc in processed_docs]"
      ],
      "metadata": {
        "id": "VXb9Z8fGB99j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary representation of the documents\n",
        "dictionary = gensim.corpora.Dictionary(bigram_docs)\n",
        "\n",
        "# Filter out extremes to limit the number of features\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
        "\n",
        "# Create a bag-of-words corpus\n",
        "corpus = [dictionary.doc2bow(text) for text in bigram_docs]\n"
      ],
      "metadata": {
        "id": "9GxJ6nu-6jdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid search best hyperparameters for LDA model using coherence_score\n",
        "def compute_coherence_values(dictionary, corpus, processed_docs, limit=10, start=5, step=1):\n",
        "    coherence_values = []\n",
        "\n",
        "    best_coherence_score = 0\n",
        "    best_lda_model = None\n",
        "\n",
        "    for num_topics in range(start, limit, step):\n",
        "          # Train the LDA model\n",
        "          lda_model = LdaModel(\n",
        "            corpus=corpus,\n",
        "            id2word=dictionary,\n",
        "            chunksize=2000,\n",
        "            alpha='auto',\n",
        "            eta='auto',\n",
        "            iterations=400,\n",
        "            num_topics=num_topics,\n",
        "            passes=30,\n",
        "            eval_every=None\n",
        "          )\n",
        "\n",
        "          # Calculate the coherence score\n",
        "          coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
        "          coherence_score = coherence_model_lda.get_coherence()\n",
        "\n",
        "          if coherence_score > best_coherence_score:\n",
        "            best_coherence_score = coherence_score\n",
        "            best_lda_model = lda_model\n",
        "\n",
        "          coherence_values.append({\"num_topics\": num_topics,  \"Coherence Score\": coherence_score})\n",
        "\n",
        "    return coherence_values, best_lda_model, best_coherence_score"
      ],
      "metadata": {
        "id": "7zDVghWk685_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coherence_values, lda_model, coherence_score = compute_coherence_values(dictionary, corpus, bigram_docs)\n",
        "\n",
        "coherence_values_df = pd.DataFrame(coherence_values)\n",
        "\n",
        "# Compute Perplexity\n",
        "print('Perplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "print('Coherence Score: ', coherence_score)\n"
      ],
      "metadata": {
        "id": "_mD4Be0AxpTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the topics and their words\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f\"Topic #{idx + 1}: {topic}\")"
      ],
      "metadata": {
        "id": "Rm6vmuAP7Am2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "\n",
        "# Prepare the visualisation.\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(vis)"
      ],
      "metadata": {
        "id": "wUw_cCjsAHfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gensim - LDA - Combined - per quarter"
      ],
      "metadata": {
        "id": "vgnxqDmy-1uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\" (empty) for combined transcripts\n",
        "transcript_answers_df = get_transcripts(\"\")\n",
        "get_topics_per_quarter(transcript_answers_df)"
      ],
      "metadata": {
        "id": "uZBZnHmr-Pwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gensim - LDA - Presentation - per quarter"
      ],
      "metadata": {
        "id": "3SC8mQ2QI2y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_answers_df = get_transcripts(\"Presentation\")\n",
        "get_topics_per_quarter(transcript_answers_df)"
      ],
      "metadata": {
        "id": "040pnkWHJabD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gensim - LDA - Question-and-Answer - per quarter"
      ],
      "metadata": {
        "id": "g0_L48cXNEvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_answers_df = get_transcripts(\"Question-and-Answer\")\n",
        "get_topics_per_quarter(transcript_answers_df)"
      ],
      "metadata": {
        "id": "1oIfwPPSNJAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarised Transcript"
      ],
      "metadata": {
        "id": "XAfBDXVpV5-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gensim - LDA - Combined - per quarter"
      ],
      "metadata": {
        "id": "XqUBM3SwWD0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_answers_df = get_transcripts(\"\")\n",
        "get_topics_per_quarter(transcript_answers_df, \"Summarised_dialogue\")"
      ],
      "metadata": {
        "id": "suuPdlPKYz0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gensim - LDA - Presentation - per quarter"
      ],
      "metadata": {
        "id": "NGuMKKTIY3TA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_answers_df = get_transcripts(\"Presentation\")\n",
        "get_topics_per_quarter(transcript_answers_df, \"Summarised_dialogue\")"
      ],
      "metadata": {
        "id": "K5Ol4uy_WJ_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gensim - LDA - Question-and-Answer - per quarter"
      ],
      "metadata": {
        "id": "6aT26LF8ZZP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_answers_df = get_transcripts(\"Question-and-Answer\")\n",
        "get_topics_per_quarter(transcript_answers_df, \"Summarised_dialogue\")"
      ],
      "metadata": {
        "id": "UroKPpalZeyE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}