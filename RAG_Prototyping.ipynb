{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Based Implementation"
      ],
      "metadata": {
        "id": "UVMoqA5fmraK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pip Installs and Import Statements"
      ],
      "metadata": {
        "id": "qjtoeCRRDDA0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN3du5hU9X45"
      },
      "outputs": [],
      "source": [
        "# Install and upgrade necessary libraries for RAG Pipeline and Topic Modeling\n",
        "\n",
        "# Install core libraries for RAG pipeline\n",
        "!pip install langchain-huggingface langchain chromadb pypdf sentence-transformers accelerate langchain-community pypdf2 torch\n",
        "\n",
        "# Upgrade libraries to the latest versions\n",
        "!pip install --upgrade langchain transformers sentence-transformers langchain-huggingface chromadb langchain_chroma\n",
        "\n",
        "# Install and upgrade additional libraries\n",
        "!pip install tiktoken\n",
        "!pip install --upgrade torch torchvision torchaudio transformers sentence-transformers\n",
        "!pip install -U langchain_chroma\n",
        "!pip install -U langchain_huggingface langchain_chroma\n",
        "\n",
        "# Install libraries for topic modeling\n",
        "!pip install bertopic\n",
        "!pip install nltk bertopic\n",
        "!pip install pyLDAvis\n",
        "\n",
        "# Download Spacy English model for NER\n",
        "!python -m spacy download en_core_web_md\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw0Ja11P9X48"
      },
      "outputs": [],
      "source": [
        "# =====================================================================\n",
        "# 1. STANDARD LIBRARIES\n",
        "# =====================================================================\n",
        "import logging\n",
        "import re\n",
        "from typing import List, Dict, Set\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# =====================================================================\n",
        "# 2. THIRD-PARTY LIBRARIES\n",
        "# =====================================================================\n",
        "# 2.1 Data Analysis\n",
        "import pandas as pd\n",
        "\n",
        "# 2.2 Machine Learning and Deep Learning\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# 2.3 Natural Language Processing (NLP)\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "import spacy\n",
        "\n",
        "# 2.4 Visualization\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "# 2.5 Topic Modeling\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 2.6 Gensim Topic Modeling\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# =====================================================================\n",
        "# 3. LANGCHAIN MODULES\n",
        "# =====================================================================\n",
        "## 3.1 Document Management\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "## 3.2 Prompts and Chains\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "## 3.3 Embeddings and Vector Store\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# =====================================================================\n",
        "# 4. HUGGING FACE INTEGRATION\n",
        "# =====================================================================\n",
        "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
        "from transformers import pipeline as hf_pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "PFg3vSs1M5Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log configurations"
      ],
      "metadata": {
        "id": "ZdJhqKzYDKGX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGFaYWUv9X48"
      },
      "outputs": [],
      "source": [
        "# Create or get logger\n",
        "logger = logging.getLogger(\"RAGRetriever\")\n",
        "\n",
        "# Clear existing handlers\n",
        "if logger.hasHandlers():\n",
        "    logger.handlers.clear()\n",
        "\n",
        "# Disable propagation to ancestor loggers\n",
        "logger.propagate = False\n",
        "\n",
        "# Set logger level\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Formatter\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# File handler\n",
        "file_handler = logging.FileHandler(\"pdf_processing_errors.log\")\n",
        "file_handler.setFormatter(formatter)\n",
        "logger.addHandler(file_handler)\n",
        "\n",
        "# Console handler\n",
        "console_handler = logging.StreamHandler(sys.stdout)\n",
        "console_handler.setFormatter(formatter)\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "# Example usage\n",
        "logger.info(\"Logging system is configured.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnAjr00sHe3t"
      },
      "source": [
        "### Preprocessing Utilities\n",
        "\n",
        "Purpose: These functions handle text preprocessing for cleaning, tokenizing, and preparing text for embedding, summarization, or topic modeling.\n",
        "\n",
        "**Functions:**\n",
        "\n",
        "- normalize_metadata\n",
        "- normalize_metadata_values\n",
        "- preprocess_text\n",
        "- preprocess_text_gensim\n",
        "- preprocess_corpus_gensim\n",
        "- preprocess_corpus_bertopic\n",
        "- remove_repetitions\n",
        "- get_page_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3Jn7SIsHe3t"
      },
      "outputs": [],
      "source": [
        "# Download NLTK stopwords\n",
        "download(\"stopwords\")\n",
        "DEFAULT_STOPWORDS = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Load Spacy NLP model for NER\n",
        "nlp_model = spacy.load(\"en_core_web_md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqFy1nLSHe3t"
      },
      "outputs": [],
      "source": [
        "def normalize_metadata_values(metadata: Dict, required_fields: Set[str], case: str = \"lower\") -> Dict:\n",
        "    \"\"\"\n",
        "    Normalizes metadata fields and ensures required fields are present.\n",
        "    Args:\n",
        "        metadata (dict): Metadata dictionary.\n",
        "        required_fields (set): Set of required metadata fields.\n",
        "        case (str): Case normalization ('lower', 'upper', or 'title').\n",
        "    Returns:\n",
        "        dict: Normalized metadata.\n",
        "    \"\"\"\n",
        "    normalized_metadata = {}\n",
        "\n",
        "    for key in required_fields:\n",
        "        value = metadata.get(key)\n",
        "        normalized_metadata[key] = \"N/A\" if value is None else str(value).strip().lower() if case == \"lower\" else str(value).strip().upper()\n",
        "\n",
        "    return normalized_metadata\n",
        "\n",
        "def normalize_metadata(metadata_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalizes the metadata DataFrame by standardizing text fields and handling missing values.\n",
        "    Args:\n",
        "        metadata_df (pd.DataFrame): The metadata DataFrame to normalize.\n",
        "    Returns:\n",
        "        pd.DataFrame: The normalized metadata DataFrame.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.info(\"Starting metadata normalization...\")\n",
        "\n",
        "        required_fields = {\"Year\", \"Quarter\", \"Bank\", \"Designation\", \"Name\", \"Source\"}\n",
        "\n",
        "        # Check for required columns\n",
        "        missing_columns = required_fields - set(metadata_df.columns)\n",
        "        if missing_columns:\n",
        "            raise KeyError(f\"Metadata DataFrame is missing required columns: {missing_columns}\")\n",
        "\n",
        "        # Normalize all fields using `normalize_metadata_values`\n",
        "        normalized_metadata = metadata_df.apply(\n",
        "            lambda row: normalize_metadata_values(row.to_dict(), required_fields, case=\"lower\"), axis=1\n",
        "        )\n",
        "        normalized_df = pd.DataFrame(list(normalized_metadata))\n",
        "\n",
        "        logger.info(f\"Metadata normalization completed successfully for {len(metadata_df)} rows.\")\n",
        "        return normalized_df\n",
        "\n",
        "    except KeyError as ke:\n",
        "        logger.error(f\"Metadata normalization error: {ke}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error during metadata normalization: {e}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmDS3c75He3u"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(\n",
        "    text: str,\n",
        "    nlp_model=None,\n",
        "    custom_stopwords=None,\n",
        "    redundant_terms=None\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Preprocess text for BERTopic.\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw text input.\n",
        "        nlp_model (spacy.Language): SpaCy model for NER.\n",
        "        custom_stopwords (list): Additional stopwords.\n",
        "        redundant_terms (list): Terms to explicitly remove.\n",
        "\n",
        "    Returns:\n",
        "        list: Tokenized and cleaned words.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Normalize text\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
        "\n",
        "        # Remove emails\n",
        "        text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "\n",
        "        # Use NER to remove names\n",
        "        if nlp_model:\n",
        "            doc = nlp_model(text)\n",
        "            filtered_tokens = [token.text for token in doc if token.ent_type_ != \"PERSON\"]\n",
        "            text = \" \".join(filtered_tokens)\n",
        "\n",
        "        # Default redundant terms\n",
        "        default_redundant_terms = [\n",
        "            \"thank you\", \"good morning\", \"good afternoon\", \"earnings call\",\n",
        "            \"conference call\", \"slide\", \"question\", \"operator\"\n",
        "        ]\n",
        "        redundant_terms = set(redundant_terms or []).union(default_redundant_terms)\n",
        "\n",
        "        # Remove redundant terms\n",
        "        for term in redundant_terms:\n",
        "            text = re.sub(rf\"\\b{re.escape(term)}\\b\", \"\", text)\n",
        "\n",
        "        # Remove special characters and normalize whitespace\n",
        "        text = re.sub(r\"[^a-z\\s]\", \"\", text).strip()\n",
        "        text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "        # Tokenize and remove stopwords\n",
        "        stop_words = DEFAULT_STOPWORDS.union(custom_stopwords or [])\n",
        "        tokens = [word for word in text.split() if word not in stop_words]\n",
        "\n",
        "        return tokens\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during text preprocessing: {e}\")\n",
        "        return []\n",
        "\n",
        "def preprocess_corpus_bertopic(\n",
        "    corpus, nlp_model=None, custom_stopwords=None, redundant_terms=None, min_count=2, threshold=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Preprocess a corpus for BERTopic and optionally generate bigrams and trigrams.\n",
        "\n",
        "    Args:\n",
        "        corpus (list of str): List of raw text documents.\n",
        "        nlp_model (spacy.Language): Spacy NER model for removing names (PERSON entities).\n",
        "        custom_stopwords (list): Additional stopwords to remove.\n",
        "        redundant_terms (list): Terms to explicitly remove from text.\n",
        "        min_count (int): Minimum count for bigram detection.\n",
        "        threshold (int): Phrase scoring threshold for bigram detection.\n",
        "\n",
        "    Returns:\n",
        "        list of str: Preprocessed text documents (joined tokens).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting corpus preprocessing for BERTopic...\")\n",
        "\n",
        "    def remove_urls(text):\n",
        "        \"\"\"Remove URLs from the text.\"\"\"\n",
        "        return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
        "\n",
        "    # Preprocess each document in the corpus\n",
        "    logger.info(\"Preprocessing individual documents...\")\n",
        "    tokenized_corpus = []\n",
        "    for doc in corpus:\n",
        "        # Normalize text and remove URLs\n",
        "        doc = remove_urls(doc)\n",
        "\n",
        "        # Tokenize, filter stopwords, and remove redundant terms\n",
        "        tokens = preprocess_text(doc, nlp_model, custom_stopwords, redundant_terms)\n",
        "        tokenized_corpus.append(tokens)\n",
        "\n",
        "    # Log tokenized sample\n",
        "    logger.info(\"Sample preprocessed tokens:\")\n",
        "    logger.info(tokenized_corpus[:2])\n",
        "\n",
        "    # Build bigram and trigram models with adjusted parameters\n",
        "    logger.info(\"Building bigram and trigram models...\")\n",
        "    bigram_model = Phrases(tokenized_corpus, min_count=min_count, threshold=threshold)\n",
        "    trigram_model = Phrases(bigram_model[tokenized_corpus], threshold=threshold)\n",
        "    bigram_phraser = Phraser(bigram_model)\n",
        "    trigram_phraser = Phraser(trigram_model)\n",
        "\n",
        "    # Apply bigram and trigram models to the corpus\n",
        "    logger.info(\"Applying bigram and trigram models...\")\n",
        "    processed_corpus = []\n",
        "    for doc in tokenized_corpus:\n",
        "        phrases = trigram_phraser[bigram_phraser[doc]]\n",
        "        # Join tokens back into strings for BERTopic\n",
        "        processed_corpus.append(\" \".join(phrases))\n",
        "\n",
        "    # Log sample processed corpus\n",
        "    logger.info(\"Sample processed documents for BERTopic:\")\n",
        "    logger.info(processed_corpus[:2])\n",
        "\n",
        "    return processed_corpus\n",
        "\n",
        "\n",
        "def preprocess_text_gensim(text, nlp_model=None, custom_stopwords=None, redundant_terms=None):\n",
        "    \"\"\"\n",
        "    Preprocess text for LDA topic modeling, focusing on bigrams and trigrams.\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw text to preprocess.\n",
        "        nlp_model (spacy.Language): Spacy NER model for removing names (PERSON entities).\n",
        "        custom_stopwords (list): Additional stopwords to remove.\n",
        "        redundant_terms (list): Terms to explicitly remove from text.\n",
        "\n",
        "    Returns:\n",
        "        list: Tokenized and preprocessed words.\n",
        "    \"\"\"\n",
        "    # Normalize text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
        "\n",
        "    # Remove names using Spacy NER if model is providedz\n",
        "    if nlp_model:\n",
        "        doc = nlp_model(text)\n",
        "        filtered_tokens = [token.text for token in doc if token.ent_type_ != \"PERSON\"]\n",
        "        text = \" \".join(filtered_tokens)\n",
        "\n",
        "    # Default redundant terms to remove\n",
        "    default_redundant_terms = [\n",
        "        \"seeking_alpha\", \"thank_much\", \"group_ag_cs_results\", \"transcript_seeking_alpha\",\n",
        "        \"good_morning\", \"good_afternoon\", \"earnings_call\", \"company\", \"presentation\",\n",
        "        \"analyst\", \"operator\", \"thomas\", \"gottstein\"\n",
        "    ]\n",
        "\n",
        "    # Merge default and custom redundant terms\n",
        "    if redundant_terms:\n",
        "        redundant_terms.extend(default_redundant_terms)\n",
        "    else:\n",
        "        redundant_terms = default_redundant_terms\n",
        "\n",
        "    # Remove redundant terms using regex\n",
        "    for term in redundant_terms:\n",
        "        text = re.sub(rf\"\\b{re.escape(term.lower())}\\b\", \"\", text)\n",
        "\n",
        "    # Remove special characters, digits, and extra whitespace\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # Remove non-alphabetic characters\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize whitespace\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    stop_words = DEFAULT_STOPWORDS\n",
        "    if custom_stopwords:\n",
        "        stop_words.update(custom_stopwords)\n",
        "\n",
        "    tokens = [word for word in text.split() if word not in stop_words]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def preprocess_corpus_gensim(\n",
        "    corpus, nlp_model=None, custom_stopwords=None, redundant_terms=None, extra_redundant_terms=None, min_count=2, threshold=5, keep_unigrams=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Preprocess a corpus for LDA topic modeling and generate bigrams and trigrams.\n",
        "\n",
        "    Args:\n",
        "        corpus (list of str): List of raw text documents.\n",
        "        nlp_model (spacy.Language): Spacy NER model for removing names (PERSON entities).\n",
        "        custom_stopwords (list): Additional stopwords to remove.\n",
        "        redundant_terms (list): Terms to explicitly remove from text.\n",
        "        min_count (int): Minimum count for bigram/trigram detection.\n",
        "        threshold (int): Phrase scoring threshold for bigram/trigram detection.\n",
        "        keep_unigrams (bool): If True, include unigrams alongside bigrams/trigrams.\n",
        "\n",
        "    Returns:\n",
        "        list of list: Preprocessed tokenized documents with bigrams/trigrams (and optionally unigrams).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting corpus preprocessing for bigrams and trigrams...\")\n",
        "\n",
        "    def remove_urls(text):\n",
        "        \"\"\"Remove URLs from the text.\"\"\"\n",
        "        return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
        "\n",
        "    # Preprocess each document in the corpus\n",
        "    logger.info(\"Preprocessing individual documents...\")\n",
        "    tokenized_corpus = []\n",
        "    for doc in corpus:\n",
        "        # Normalize text and remove URLs\n",
        "        doc = remove_urls(doc)\n",
        "\n",
        "        # Tokenize, filter stopwords, and remove redundant terms\n",
        "        tokens = preprocess_text_gensim(doc, nlp_model, custom_stopwords, redundant_terms)\n",
        "        tokenized_corpus.append(tokens)\n",
        "\n",
        "    # Log tokenized sample\n",
        "    logger.info(\"Sample preprocessed tokens:\")\n",
        "    logger.info(tokenized_corpus[:2])\n",
        "\n",
        "    # Build bigram and trigram models with adjusted parameters\n",
        "    logger.info(\"Building bigram and trigram models...\")\n",
        "    bigram_model = Phrases(tokenized_corpus, min_count=min_count, threshold=threshold)\n",
        "    trigram_model = Phrases(bigram_model[tokenized_corpus], threshold=threshold)\n",
        "    bigram_phraser = Phraser(bigram_model)\n",
        "    trigram_phraser = Phraser(trigram_model)\n",
        "\n",
        "    # Apply bigram and trigram models to the corpus\n",
        "    logger.info(\"Applying bigram and trigram models...\")\n",
        "    processed_corpus = []\n",
        "    for doc in tokenized_corpus:\n",
        "        phrases = trigram_phraser[bigram_phraser[doc]]\n",
        "        if keep_unigrams:\n",
        "            # Include unigrams alongside bigrams/trigrams\n",
        "            processed_corpus.append(list(phrases))\n",
        "        else:\n",
        "            # Keep only bigrams/trigrams\n",
        "            processed_corpus.append([token for token in phrases if \"_\" in token])\n",
        "\n",
        "    # Log sample processed corpus\n",
        "    logger.info(\"Sample processed documents with bigrams/trigrams:\")\n",
        "    logger.info(processed_corpus[:2])\n",
        "\n",
        "    return processed_corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41zcyOPOHe3u"
      },
      "source": [
        "### Pipeline Configuration\n",
        "\n",
        "Purpose: These functions set up the overall Retrieval-Augmented Generation (RAG) pipeline, including metadata handling, document loading, embedding generation, and retriever initialization.\n",
        "\n",
        "**Functions:**\n",
        "- load_metadata\n",
        "- load_pdf_file\n",
        "- load_documents\n",
        "- attach_metadata_to_documents\n",
        "- propagate_metadata_to_chunks\n",
        "- validate_metadata\n",
        "- initialize_embeddings_and_vector_store\n",
        "- setup_retriever\n",
        "- load_model\n",
        "- configure_rag_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgVhadqSHe3v"
      },
      "outputs": [],
      "source": [
        "def load_metadata(metadata_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads and normalizes metadata from the specified CSV file.\n",
        "    Args:\n",
        "        metadata_path (str): Path to the metadata CSV file.\n",
        "    Returns:\n",
        "        pd.DataFrame: The normalized metadata DataFrame.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.info(f\"Loading metadata from: {metadata_path}\")\n",
        "\n",
        "        metadata_df = pd.read_csv(metadata_path)\n",
        "        logger.info(f\"Loaded {len(metadata_df)} rows of metadata.\")\n",
        "\n",
        "        # Rename 'File' column to 'Source' if necessary\n",
        "        if \"File\" in metadata_df.columns:\n",
        "            metadata_df.rename(columns={\"File\": \"Source\"}, inplace=True)\n",
        "\n",
        "        # Normalize the metadata\n",
        "        metadata_df = normalize_metadata(metadata_df)\n",
        "\n",
        "        logger.info(f\"Loaded data with meta data completed successfully for {len(metadata_df)} rows.\")\n",
        "        return metadata_df\n",
        "\n",
        "    except pd.errors.EmptyDataError:\n",
        "        logger.error(\"The metadata file is empty or improperly formatted.\")\n",
        "        raise ValueError(\"The metadata file is empty or improperly formatted.\")\n",
        "    except KeyError as ke:\n",
        "        logger.error(f\"Metadata normalization error: {ke}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error during metadata loading: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def load_pdf_file(file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Loads and splits a single PDF file into document objects.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: A list of document objects with metadata.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        pdf_documents = loader.load_and_split()\n",
        "        logger.info(f\"Loaded {len(pdf_documents)} pages from {file_path}\")\n",
        "\n",
        "        # Assign normalized source metadata to each document\n",
        "        for doc in pdf_documents:\n",
        "            doc.metadata = {\"source\": os.path.basename(file_path).strip().lower()}\n",
        "\n",
        "        return pdf_documents\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading or processing PDF '{file_path}': {e}\")\n",
        "        return []\n",
        "\n",
        "def load_documents(folder_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Loads PDF documents from the specified folder.\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder containing PDF files.\n",
        "    Returns:\n",
        "        List[Document]: A list of document objects with metadata.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(folder_path):\n",
        "            raise FileNotFoundError(f\"Specified folder path does not exist: {folder_path}\")\n",
        "\n",
        "        logger.info(f\"Loading PDF files from folder: {folder_path}\")\n",
        "\n",
        "        documents = [\n",
        "            doc\n",
        "            for filename in os.listdir(folder_path)\n",
        "            if filename.lower().endswith(\".pdf\")\n",
        "            for doc in load_pdf_file(os.path.join(folder_path, filename))\n",
        "        ]\n",
        "\n",
        "        if not documents:\n",
        "            logger.warning(\"No valid PDF documents were loaded from the folder.\")\n",
        "        else:\n",
        "            logger.info(f\"Successfully loaded {len(documents)} documents from {folder_path}.\")\n",
        "\n",
        "        return documents\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred while loading documents: {e}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Utr_IIwwHe3v"
      },
      "outputs": [],
      "source": [
        "def attach_metadata_to_documents(documents: List[Document], metadata_df: pd.DataFrame) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Attaches metadata to documents based on their source.\n",
        "    Args:\n",
        "        documents (List[Document]): List of document objects.\n",
        "        metadata_df (pd.DataFrame): Normalized metadata DataFrame.\n",
        "    Returns:\n",
        "        List[Document]: List of documents with enriched metadata.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if \"Source\" not in metadata_df.columns:\n",
        "            raise KeyError(\"The metadata DataFrame is missing the 'Source' column.\")\n",
        "\n",
        "        enriched_documents = []\n",
        "\n",
        "        for doc in documents:\n",
        "            source_file = os.path.basename(doc.metadata.get(\"source\", \"\")).lower().strip()\n",
        "            logger.debug(f\"Processing document with source: {source_file}\")\n",
        "\n",
        "            matched_metadata = metadata_df[metadata_df[\"Source\"] == source_file]\n",
        "\n",
        "            if not matched_metadata.empty:\n",
        "                metadata_dict = matched_metadata.iloc[0].to_dict()\n",
        "                doc.metadata = {\n",
        "                    key.lower(): str(value).strip() if pd.notna(value) else \"n/a\"\n",
        "                    for key, value in metadata_dict.items()\n",
        "                }\n",
        "            else:\n",
        "                logger.warning(f\"No metadata match found for source: {source_file}\")\n",
        "\n",
        "            enriched_documents.append(doc)\n",
        "\n",
        "        return enriched_documents\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error attaching metadata to documents: {e}\")\n",
        "        raise\n",
        "\n",
        "def propagate_metadata_to_chunks(documents: List[Document], chunk_size=512, chunk_overlap=50) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Splits documents into token-based chunks and propagates normalized metadata to each chunk.\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of documents with metadata.\n",
        "        chunk_size (int): Maximum number of tokens per chunk.\n",
        "        chunk_overlap (int): Number of overlapping tokens between chunks.\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: A list of document chunks with normalized metadata.\n",
        "    \"\"\"\n",
        "    # Token-based text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "    # Load tokenizer for token validation\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "    enriched_chunks = []\n",
        "\n",
        "    for doc_idx, doc in enumerate(documents):\n",
        "        # Split document into token-based chunks\n",
        "        chunks = text_splitter.split_documents([doc])\n",
        "\n",
        "        for chunk_idx, chunk in enumerate(chunks):\n",
        "            # Propagate metadata to the chunk\n",
        "            chunk.metadata.update(doc.metadata)\n",
        "            chunk.metadata[\"chunk_id\"] = f\"{doc.metadata.get('source', 'unknown')}_{doc_idx}_{chunk_idx}\"\n",
        "\n",
        "            # Estimate token count and log\n",
        "            token_count = len(tokenizer.encode(chunk.page_content, truncation=False))\n",
        "            logger.debug(f\"Chunk {chunk_idx} from Document {doc_idx} has {token_count} tokens.\")\n",
        "            if token_count > 1024:\n",
        "                logger.warning(f\"Chunk {chunk_idx} exceeds token limit with {token_count} tokens.\")\n",
        "\n",
        "            enriched_chunks.append(chunk)\n",
        "\n",
        "    logger.info(f\"Generated {len(enriched_chunks)} enriched chunks.\")\n",
        "    return enriched_chunks\n",
        "\n",
        "def validate_metadata(documents: List[Document], required_fields: Set[str] = None) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Validates and standardizes metadata for a list of documents.\n",
        "    \"\"\"\n",
        "    if required_fields is None:\n",
        "        required_fields = {\"year\", \"quarter\", \"bank\", \"designation\", \"name\", \"source\"}\n",
        "\n",
        "    validated_documents = []\n",
        "    try:\n",
        "        for doc in documents:\n",
        "            if hasattr(doc, \"metadata\") and isinstance(doc.metadata, dict):\n",
        "                for field in required_fields:\n",
        "                    if field not in doc.metadata or not doc.metadata[field]:\n",
        "                        doc.metadata[field] = \"Unknown\"\n",
        "                validated_documents.append(doc)\n",
        "        return validated_documents\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in validate_metadata: {e}\")\n",
        "        return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be5gcPgUHe3v"
      },
      "outputs": [],
      "source": [
        "def initialize_embeddings_and_vector_store(\n",
        "    chunks,\n",
        "    embeddings_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    persist_directory=\"qa_index\",\n",
        "    use_existing=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Initializes embeddings and vector store.\n",
        "\n",
        "    Args:\n",
        "        chunks (list): List of document chunks for embedding.\n",
        "        embeddings_model_name (str): Hugging Face model to generate embeddings.\n",
        "        persist_directory (str): Path to persist or load the vector store.\n",
        "        use_existing (bool): If True, load an existing vector store if available.\n",
        "\n",
        "    Returns:\n",
        "        Chroma: Initialized or loaded vector store instance.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not chunks:\n",
        "            raise ValueError(\"No chunks provided for vector store initialization.\")\n",
        "\n",
        "        # Ensure the persist directory exists\n",
        "        os.makedirs(persist_directory, exist_ok=True)\n",
        "        logger.info(f\"Using embedding model: {embeddings_model_name}\")\n",
        "\n",
        "        # Initialize embeddings using the specified model\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
        "\n",
        "        # Check if an existing vector store should be used\n",
        "        vector_store = None\n",
        "        if use_existing and os.path.exists(persist_directory) and os.listdir(persist_directory):\n",
        "            try:\n",
        "                logger.info(f\"Loading existing vector store from: {persist_directory}\")\n",
        "                vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "                # Check if the vector store contains any embeddings\n",
        "                if vector_store._collection.count() == 0:\n",
        "                    logger.warning(\"Existing vector store is empty. Rebuilding with provided chunks...\")\n",
        "                    vector_store = Chroma.from_documents(\n",
        "                        documents=chunks, embedding=embeddings, persist_directory=persist_directory\n",
        "                    )\n",
        "                    logger.info(f\"Rebuilt vector store with {len(chunks)} document chunks.\")\n",
        "                else:\n",
        "                    logger.info(f\"Loaded vector store with {vector_store._collection.count()} embeddings.\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to load existing vector store: {e}. Rebuilding...\")\n",
        "\n",
        "        # If no vector store exists or is empty, create a new one\n",
        "        if not vector_store:\n",
        "            logger.info(f\"Creating a new vector store with {len(chunks)} document chunks.\")\n",
        "            vector_store = Chroma.from_documents(\n",
        "                documents=chunks, embedding=embeddings, persist_directory=persist_directory\n",
        "            )\n",
        "\n",
        "        logger.info(\"Vector store initialized successfully.\")\n",
        "        return vector_store\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error initializing vector store.\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def setup_retriever(vector_store: Chroma, top_k=3):\n",
        "    \"\"\"\n",
        "    Configures a retriever for the vector store with an embedding function.\n",
        "\n",
        "    Args:\n",
        "        vector_store (Chroma): The vector database instance.\n",
        "        top_k (int): Number of top results to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        Retriever: A retriever configured for the vector store.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not vector_store:\n",
        "            raise ValueError(\"Vector store is empty or not initialized.\")\n",
        "\n",
        "        logger.info(f\"Setting up retriever with top_k={top_k}...\")\n",
        "        retriever = vector_store.as_retriever(search_kwargs={\"k\": top_k})\n",
        "\n",
        "        logger.info(\"Retriever configured successfully.\")\n",
        "        return retriever\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error setting up retriever.\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsdAoVj8He3w"
      },
      "outputs": [],
      "source": [
        "def load_model(\n",
        "    model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    max_new_tokens=300,\n",
        "    device=\"cuda\",\n",
        "    precision=\"float16\",\n",
        "    trust_remote_code=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Loads a language model and sets up a text-generation pipeline.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Name of the pre-trained model to load.\n",
        "        max_new_tokens (int): Maximum number of tokens to generate per response.\n",
        "        device (str): Device to load the model on (\"cuda\" or \"cpu\").\n",
        "        precision (str): Precision type (\"float16\", \"float32\").\n",
        "        trust_remote_code (bool): Trust remote code for custom models.\n",
        "\n",
        "    Returns:\n",
        "        transformers.pipelines.Pipeline: A configured pipeline ready for text generation.\n",
        "    \"\"\"\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    try:\n",
        "        # Validate max_new_tokens\n",
        "        if not isinstance(max_new_tokens, int) or max_new_tokens <= 0 or max_new_tokens > 1024:\n",
        "            raise ValueError(\"max_new_tokens must be a positive integer and less than or equal to 1024.\")\n",
        "\n",
        "        # Check device compatibility\n",
        "        if device == \"cuda\" and not torch.cuda.is_available():\n",
        "            logger.warning(\"CUDA is not available. Falling back to CPU.\")\n",
        "            device = \"cpu\"\n",
        "            precision = \"float32\"\n",
        "\n",
        "        # Determine the dtype based on precision and device\n",
        "        dtype = torch.float16 if precision == \"float16\" and device == \"cuda\" else torch.float32\n",
        "\n",
        "        # Log initialization parameters\n",
        "        logger.info(f\"Loading model '{model_name}' on {device} with precision '{precision}' and max_new_tokens={max_new_tokens}.\")\n",
        "\n",
        "        # Load the model and tokenizer\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name, torch_dtype=dtype, trust_remote_code=trust_remote_code\n",
        "        ).to(device)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=trust_remote_code)\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # Initialize the pipeline\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            device=0 if device == \"cuda\" else -1,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "\n",
        "        # Log model details\n",
        "        model_params = sum(p.numel() for p in model.parameters())\n",
        "        logger.info(f\"Model '{model_name}' loaded successfully with {model_params:,} parameters.\")\n",
        "\n",
        "        return pipe\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Failed to load model '{model_name}': {e}\")\n",
        "        raise RuntimeError(f\"Failed to load model: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65dzuox5He3w"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_data(\n",
        "    folder_path: str,\n",
        "    metadata_path: str,\n",
        "    chunk_size: int,\n",
        "    chunk_overlap: int,\n",
        "    required_metadata_fields: Set[str]\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Streamlines the loading, metadata attachment, validation, and chunking process.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder containing documents.\n",
        "        metadata_path (str): Path to the metadata CSV file.\n",
        "        chunk_size (int): Maximum number of characters per chunk.\n",
        "        chunk_overlap (int): Number of overlapping characters between chunks.\n",
        "        required_metadata_fields (Set[str]): Required metadata fields for validation.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[Document], List[Document]]: Enriched documents and their chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.info(\"Starting the preprocessing pipeline...\")\n",
        "\n",
        "        # Step 1: Load metadata from the provided CSV file\n",
        "        logger.info(\"Step 1: Loading metadata...\")\n",
        "        metadata_df = load_metadata(metadata_path)\n",
        "        logger.info(f\"Metadata loaded successfully with {len(metadata_df)} rows.\")\n",
        "\n",
        "        # Step 2: Load PDF documents from the specified folder\n",
        "        logger.info(\"Step 2: Loading documents from the folder...\")\n",
        "        documents = load_documents(folder_path)\n",
        "        if not documents:\n",
        "            raise ValueError(\"No documents found in the specified folder.\")\n",
        "        logger.info(f\"Loaded {len(documents)} documents from {folder_path}.\")\n",
        "\n",
        "        # Step 3: Attach metadata to each document\n",
        "        logger.info(\"Step 3: Attaching metadata to documents...\")\n",
        "        enriched_documents = attach_metadata_to_documents(documents, metadata_df)\n",
        "        logger.info(f\"Metadata successfully attached to {len(enriched_documents)} documents.\")\n",
        "\n",
        "        # Step 4: Validate the metadata against required fields\n",
        "        logger.info(\"Step 4: Validating metadata for enriched documents...\")\n",
        "        validated_documents = validate_metadata(enriched_documents, required_fields=required_metadata_fields)\n",
        "        logger.info(f\"Metadata validation completed for {len(validated_documents)} documents.\")\n",
        "\n",
        "        # Step 5: Split documents into token-based chunks and propagate metadata\n",
        "        logger.info(\"Step 5: Splitting documents into chunks...\")\n",
        "        document_chunks = propagate_metadata_to_chunks(\n",
        "            validated_documents,\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap\n",
        "        )\n",
        "        logger.info(f\"Generated {len(document_chunks)} document chunks.\")\n",
        "\n",
        "        # Final Step: Log completion and return results\n",
        "        logger.info(\"Preprocessing pipeline completed successfully.\")\n",
        "        return enriched_documents, document_chunks\n",
        "\n",
        "    except FileNotFoundError as fnf_error:\n",
        "        logger.error(f\"File not found: {fnf_error}\")\n",
        "        raise  # Specific actionable error, such as missing folder or file.\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred during preprocessing: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_mJSQgNHe3x"
      },
      "outputs": [],
      "source": [
        "def get_model_token_limit(model_name: str) -> int:\n",
        "    \"\"\"\n",
        "    Dynamically retrieves the token limit of a model based on its configuration.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): The name or path of the pre-trained model.\n",
        "\n",
        "    Returns:\n",
        "        int: The maximum token limit of the model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the model configuration\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        if hasattr(model.config, \"max_position_embeddings\"):\n",
        "            return model.config.max_position_embeddings\n",
        "        else:\n",
        "            raise ValueError(f\"Model {model_name} does not specify 'max_position_embeddings'.\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to retrieve token limit for model '{model_name}': {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w4vpHnNHe3x"
      },
      "source": [
        "### RAG Pipeline Implementation\n",
        "\n",
        "The Retrieval-Augmented Generation (RAG) pipeline is an advanced framework designed to enhance natural language processing by combining retrieval-based document search with generative language models. This approach provides highly accurate and context-aware responses to user queries by integrating the following components:\n",
        "\n",
        "- Document Retrieval\n",
        "- Generative Language Model\n",
        "- Metadata and Context Management\n",
        "- Sentiment Analysis\n",
        "- Summarisation\n",
        "- Topic Modeling\n",
        "\n",
        "**configure_rag_pipeline**: this function serves as the backbone for applications requiring accurate document retrieval, summarization, sentiment classification, and topic analysis, making it an all-in-one solution for retrieval-augmented generation tasks.\n",
        "\n",
        "Workflow\n",
        "- Document Preprocessing: Load and enrich documents, then split them into manageable chunks.\n",
        "- Embedding Generation: Convert document chunks into vector embeddings and store them in a ChromaDB vector store.\n",
        "- Retriever Setup: Configure a retriever to fetch the most relevant chunks for user queries.\n",
        "- Generative Responses: Use retrieved chunks to generate responses via a language model.\n",
        "- Auxiliary Insights: Apply summarization, sentiment analysis, and topic modeling for deeper understanding of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poWR6iLtHe3x"
      },
      "outputs": [],
      "source": [
        "def configure_rag_pipeline(\n",
        "    folder_path: str,\n",
        "    metadata_path: str,\n",
        "    model_name: str = \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    embeddings_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_token_limit: int = 4096,\n",
        "    top_k: int = 3,\n",
        "    max_new_tokens: int = 500,\n",
        "    chunk_size: int = None,  # Dynamically calculated based on model token limit\n",
        "    chunk_overlap: int = 200,\n",
        "    required_metadata_fields: Set[str] = None,\n",
        "    persist_directory: str = \"qa_index\",\n",
        "    device: str = \"cuda\",\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Configures a Retrieval-Augmented Generation (RAG) pipeline.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder containing documents.\n",
        "        metadata_path (str): Path to the metadata CSV file.\n",
        "        model_name (str): Default language model to load.\n",
        "        embeddings_model_name (str): The embeddings model to use for vector store creation.\n",
        "        top_k (int): Number of top results to retrieve.\n",
        "        max_new_tokens (int): Maximum number of tokens to generate.\n",
        "        chunk_size (int): Maximum number of characters in each document chunk.\n",
        "        chunk_overlap (int): Number of overlapping characters between chunks.\n",
        "        required_metadata_fields (Set[str]): Set of required metadata fields for validation.\n",
        "        persist_directory (str): Directory for the vector store.\n",
        "        device (str): Device for model loading (e.g., \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing configured pipeline components.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Dynamically fetch the model's token limit\n",
        "        model_token_limit = get_model_token_limit(model_name)\n",
        "        logger.info(f\"Model token limit for '{model_name}': {model_token_limit}\")\n",
        "\n",
        "        # Set default required metadata fields if not provided\n",
        "        if required_metadata_fields is None:\n",
        "            required_metadata_fields = {\"year\", \"quarter\", \"bank\", \"source\", \"designation\", \"name\"}\n",
        "\n",
        "        # Calculate dynamic chunk size if not specified\n",
        "        if chunk_size is None:\n",
        "            prompt_tokens = 200  # Reserve tokens for the prompt\n",
        "            token_budget = model_token_limit - prompt_tokens - max_new_tokens\n",
        "            chunk_size = min(2000, token_budget // 2)  # Use half the remaining tokens per chunk\n",
        "            logger.info(\n",
        "                f\"Dynamic chunk size calculated based on model_token_limit={model_token_limit}, \"\n",
        "                f\"prompt_tokens={prompt_tokens}, max_new_tokens={max_new_tokens}: {chunk_size}\"\n",
        "            )\n",
        "\n",
        "        # Step 1: Load and preprocess documents\n",
        "        logger.info(\"Starting document preprocessing...\")\n",
        "        enriched_documents, document_chunks = load_and_preprocess_data(\n",
        "            folder_path=folder_path,\n",
        "            metadata_path=metadata_path,\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            required_metadata_fields=required_metadata_fields,\n",
        "        )\n",
        "        if not enriched_documents:\n",
        "            logger.error(\"No documents were enriched. Check metadata and folder path.\")\n",
        "            return None\n",
        "\n",
        "        if not document_chunks:\n",
        "            logger.error(\"No document chunks generated. Check text splitting or preprocessing logic.\")\n",
        "            return None\n",
        "\n",
        "        # Log enriched document metadata for verification\n",
        "        logger.info(\"Enriched document metadata (preview):\")\n",
        "        for i, doc in enumerate(enriched_documents[:3]):  # Preview the first 3 documents\n",
        "            logger.info(f\"Document {i + 1}: {doc.metadata}\")\n",
        "\n",
        "        # Step 2: Initialize embeddings and vector store\n",
        "        logger.info(\"Initializing embeddings and vector store...\")\n",
        "        vector_store = initialize_embeddings_and_vector_store(\n",
        "            chunks=document_chunks,\n",
        "            embeddings_model_name=embeddings_model_name,\n",
        "            persist_directory=persist_directory,\n",
        "            use_existing=True,\n",
        "        )\n",
        "\n",
        "        if not vector_store or vector_store._collection.count() == 0:\n",
        "            logger.warning(\"Vector store is empty. Consider rebuilding with valid document chunks.\")\n",
        "            return None\n",
        "\n",
        "        # Step 3: Set up retriever\n",
        "        logger.info(\"Setting up retriever...\")\n",
        "        retriever = setup_retriever(vector_store, top_k=top_k)\n",
        "        if not retriever:\n",
        "            logger.error(\"Failed to configure retriever. Exiting pipeline configuration.\")\n",
        "            return None\n",
        "\n",
        "        # Step 4: Load the language model\n",
        "        logger.info(\"Loading the language model...\")\n",
        "        llm_pipeline = load_model(\n",
        "            model_name=model_name,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        if not llm_pipeline:\n",
        "            logger.error(\"Failed to load the language model. Exiting pipeline configuration.\")\n",
        "            return None\n",
        "\n",
        "        # Step 5: Initialize QA chains\n",
        "        logger.info(\"Initializing QA chains...\")\n",
        "        qa_chains = initialize_chains(llm_pipeline, \"stuff\")\n",
        "\n",
        "        if not qa_chains:\n",
        "            logger.error(\"Failed to initialize QA chains. Exiting pipeline configuration.\")\n",
        "            return None\n",
        "\n",
        "        # Step 6: Load metadata for display and options\n",
        "        logger.info(\"Loading metadata...\")\n",
        "        metadata_df = pd.read_csv(metadata_path)\n",
        "        if metadata_df.empty:\n",
        "            logger.warning(\"Metadata file is empty. Ensure the metadata CSV is correctly populated.\")\n",
        "        else:\n",
        "            logger.info(f\"Loaded metadata with {len(metadata_df)} rows.\")\n",
        "\n",
        "        # Step 7: Set up the summarization pipeline\n",
        "        logger.info(\"Initializing summarization pipeline...\")\n",
        "        summarization_pipeline = hf_pipeline(\n",
        "            \"summarization\",\n",
        "            model=\"sshleifer/distilbart-cnn-6-6\",  # Or any suitable summarization model\n",
        "            device=0 if device == \"cuda\" else -1  # Use GPU if available\n",
        "        )\n",
        "\n",
        "        # Step 8: Set up the sentiment analysis pipeline\n",
        "        logger.info(\"Initializing sentiment analysis pipeline...\")\n",
        "        sentiment_pipeline = hf_pipeline(\n",
        "            \"sentiment-analysis\",\n",
        "            model=\"yiyanghkust/finbert-tone\",  # Example sentiment analysis model\n",
        "            device=0 if device == \"cuda\" else -1  # Use GPU if available\n",
        "        )\n",
        "\n",
        "        # Step 9: Initialize BERTopic for topic modeling\n",
        "        logger.info(\"Initializing BERTopic model...\")\n",
        "        topic_model = BERTopic(language=\"english\", verbose=True)\n",
        "        logger.info(\"BERTopic model initialized successfully.\")\n",
        "\n",
        "        # Return all components in a dictionary\n",
        "        logger.info(\"RAG pipeline configured successfully.\")\n",
        "        return {\n",
        "            \"retriever\": retriever,\n",
        "            \"vector_store\": vector_store,\n",
        "            \"llm_pipeline\": llm_pipeline,\n",
        "            \"qa_chains\": qa_chains,\n",
        "            \"metadata_df\": metadata_df,\n",
        "            \"documents\": enriched_documents,\n",
        "            \"summarization_pipeline\": summarization_pipeline,\n",
        "            \"sentiment_pipeline\": sentiment_pipeline,\n",
        "            \"topic_model\": topic_model,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"An unexpected error occurred during pipeline configuration: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This pipeline can be enhanced by integrating topics and sentiments extracted via sentiment-modelling pipeline and topic-modelling pipeline. The same data can be added to the context for better and accurate results."
      ],
      "metadata": {
        "id": "DFcxxWjqDZr9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghas6njjHe3y"
      },
      "source": [
        "### RAG Pipeline and Other Helper Functions\n",
        "\n",
        "Purpose: General-purpose functions to support RAG workflows, such as extracting responses, displaying text, and dynamic user interactions.\n",
        "**Functions:**\n",
        "- select_model\n",
        "- extract_answer\n",
        "- initialize_chains\n",
        "- display_wrapped_output\n",
        "- display_title\n",
        "- display_query_menu\n",
        "- get_filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaPPPassHe3y"
      },
      "outputs": [],
      "source": [
        "def select_model(model_name=\"microsoft/Phi-3-mini-4k-instruct\"):\n",
        "    \"\"\"\n",
        "    Allows the user to select a language model dynamically.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Default model name to use if the user skips selection.\n",
        "\n",
        "    Returns:\n",
        "        str: Name of the selected model or the default model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define supported models\n",
        "    supported_models = {\n",
        "        \"1\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "        \"2\": \"microsoft/Phi-3.5-MoE-instruct\",\n",
        "        \"3\": \"gemini-1.5-flash-8b\",\n",
        "        \"4\": \"openai/gpt-4\"\n",
        "    }\n",
        "\n",
        "    # Display available models\n",
        "    print(\"\\nAvailable Language Models:\")\n",
        "    for key, model_name in supported_models.items():\n",
        "        print(f\"{key}. {model_name}\")\n",
        "\n",
        "    default_model = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "    # Prompt user for selection\n",
        "    while True:\n",
        "        try:\n",
        "            choice = input(\n",
        "                f\"\\nSelect a model (1-{len(supported_models)}) or press Enter for default [{default_model}]: \"\n",
        "            ).strip()\n",
        "\n",
        "            # Use default model if input is empty\n",
        "            if not choice:\n",
        "                logger.info(f\"Default model selected: {model_name}\")\n",
        "                print(f\"[INFO] Using default model: {model_name}\")\n",
        "                return model_name\n",
        "\n",
        "            # Validate and return the selected model\n",
        "            if choice in supported_models:\n",
        "                selected_model = supported_models[choice]\n",
        "                logger.info(f\"User selected model: {selected_model}\")\n",
        "                print(f\"[INFO] Selected model: {selected_model}\")\n",
        "\n",
        "                return selected_model\n",
        "\n",
        "            # Handle invalid input\n",
        "            logger.warning(f\"Invalid choice entered: {choice}\")\n",
        "            print(\"[ERROR] Invalid choice. Please select a valid option.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"An error occurred during model selection: {e}\")\n",
        "            print(\"[ERROR] An unexpected error occurred. Please try again.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KnTvZ13He3z"
      },
      "outputs": [],
      "source": [
        "def extract_answer(response: dict, tag: str = \"<|assistant|>\") -> str:\n",
        "    \"\"\"\n",
        "    Extracts and formats the answer from the response.\n",
        "\n",
        "    Args:\n",
        "        response (dict | str): The response dictionary or string from the QA chain.\n",
        "        tag (str): The delimiter tag to locate the assistant's response.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted answer, or a default message if not found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if response is a string\n",
        "        if isinstance(response, str):\n",
        "            response_text = response\n",
        "        elif isinstance(response, dict):\n",
        "            response_text = response.get(\"output_text\", \"No response generated.\")\n",
        "        else:\n",
        "            raise ValueError(\"Response must be a dictionary or a string.\")\n",
        "\n",
        "        if tag in response_text:\n",
        "            # Extract the text after the assistant tag\n",
        "            answer = response_text.split(tag)[-1].strip()\n",
        "        else:\n",
        "            # Default to the entire response or an error message\n",
        "            answer = response_text.strip()\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting answer: {e}\")\n",
        "        return \"Error extracting the answer.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Template\n",
        "\n",
        "This function contains a dictionary of prompt templates that are integrated to the handlers to action the menu options."
      ],
      "metadata": {
        "id": "EUxV-Ea-HuNY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0w9dNnDjHe3z"
      },
      "outputs": [],
      "source": [
        "def initialize_chains(llm_pipeline, chain_type=\"stuff\"):\n",
        "    \"\"\"\n",
        "    Initializes QA chains using predefined prompt templates for different menu options.\n",
        "\n",
        "    Args:\n",
        "        llm_pipeline: A Hugging Face pipeline wrapped for LangChain.\n",
        "        chain_type (str): The chain type to use for initializing the QA chains.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of QA chains, keyed by chain purpose.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the chain initialization fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Wrap the LLM pipeline for LangChain\n",
        "        llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "\n",
        "        # Define prompt templates\n",
        "        prompt_templates = {\n",
        "            \"generic_question\": PromptTemplate(\n",
        "                template=\"\"\"<|system|>\n",
        "                You are a senior investment analyst at a major hedge fund.\n",
        "                Your expertise lies in analyzing financial institutions with a focus on European banks.\n",
        "                You have 20+ years of experience in financial modeling and due diligence.\n",
        "                Only use the provided context and metadata to answer the question. Avoid any assumptions or unsupported claims.\n",
        "                If information is missing or incomplete, state it clearly and suggest what additional data would help.\n",
        "\n",
        "                Metadata:\n",
        "                {metadata}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                <|assistant|>\"\"\",\n",
        "                input_variables=[\"metadata\", \"context\", \"question\"]\n",
        "            ),\n",
        "            \"compare_two_quarters_same_year\": PromptTemplate(\n",
        "                template=\"\"\"<|system|>\n",
        "                You are a senior investment analyst at a major hedge fund.\n",
        "                Your goal is to identify key highlights, challenges, and strategic shifts between two quarters in same year.\n",
        "                Use only the given context and metadata. If data is unavailable, state this explicitly.\n",
        "\n",
        "\n",
        "                Based on the provided data:\n",
        "                1. Summarize the key highlights of the companys performance.\n",
        "                2. Identify challenges or risks mentioned.\n",
        "                3. Provide insights into future strategies or guidance.\n",
        "\n",
        "                Provide the output as:\n",
        "                1. Key Highlights:\n",
        "                  - [Performance overview]\n",
        "                  - [Growth areas]\n",
        "                2. Challenges/Risks:\n",
        "                  - [Key risks]\n",
        "                3. Future Strategies:\n",
        "                  - [Summary of plans or initiatives]\n",
        "\n",
        "                Metadata:\n",
        "                - Year: {year}\n",
        "                - Quarter 1: {quarter1}\n",
        "                - Quarter 2: {quarter2}\n",
        "                - Bank: {bank}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                <|assistant|>\"\"\",\n",
        "                input_variables=[\"context\", \"question\", \"year\", \"quarter1\", \"quarter2\", \"bank\"]\n",
        "            ),\n",
        "            \"compare_two_quarters_diff_years\": PromptTemplate(\n",
        "                template=\"\"\"<|system|>\n",
        "                You are a senior investment analyst at a major hedge fund.\n",
        "                Your goal is to identify key highlights, challenges, and strategic shifts across two quarters in different years.\n",
        "                Use only the given context and metadata. If data is unavailable, state this explicitly.\n",
        "\n",
        "\n",
        "                Based on the provided data:\n",
        "                1. Summarize the key highlights of the companys performance.\n",
        "                2. Identify challenges or risks mentioned.\n",
        "                3. Provide insights into future strategies or guidance.\n",
        "\n",
        "                Provide the output as:\n",
        "                1. Key Highlights:\n",
        "                  - [Performance overview]\n",
        "                  - [Growth areas]\n",
        "                2. Challenges/Risks:\n",
        "                  - [Key risks]\n",
        "                3. Future Strategies:\n",
        "                  - [Summary of plans or initiatives]\n",
        "\n",
        "                Metadata:\n",
        "                - Quarter 1: {quarter1} ({year1})\n",
        "                - Quarter 2: {quarter2} ({year2})\n",
        "                - Bank: {bank}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                <|assistant|>\"\"\",\n",
        "                input_variables=[\"context\", \"question\", \"year1\", \"year2\", \"quarter1\", \"quarter2\", \"bank\"]\n",
        "            ),\n",
        "            \"year_comparison\": PromptTemplate(\n",
        "                template=\"\"\"<|system|>\n",
        "                You are a senior investment analyst at a major hedge fund.\n",
        "                Your goal is to identify key highlights, challenges, and strategic shifts across years.\n",
        "                Use only the given context and metadata. If data is unavailable, state this explicitly.\n",
        "\n",
        "\n",
        "                Based on the provided data:\n",
        "                1. Summarize the key highlights of the companys performance.\n",
        "                2. Identify challenges or risks mentioned.\n",
        "                3. Provide insights into future strategies or guidance.\n",
        "\n",
        "                Provide the output as:\n",
        "                1. Key Highlights:\n",
        "                  - [Performance overview]\n",
        "                  - [Growth areas]\n",
        "                2. Challenges/Risks:\n",
        "                  - [Key risks]\n",
        "                3. Future Strategies:\n",
        "                  - [Summary of plans or initiatives]\n",
        "\n",
        "                Metadata:\n",
        "                - Year 1: {year1}\n",
        "                - Year 2: {year2}\n",
        "                - Bank: {bank}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                <|assistant|>\"\"\",\n",
        "                input_variables=[\"context\", \"question\", \"year1\", \"year2\", \"bank\"]\n",
        "            ),\n",
        "            \"all_quarters_same_year\": PromptTemplate(\n",
        "                template=\"\"\"<|system|>\n",
        "                You are a senior investment analyst at a major hedge fund.\n",
        "                Your goal is to identify key highlights, challenges, and strategic shifts across all quarters in a year.\n",
        "                Use only the given context and metadata. If data is unavailable, state this explicitly.\n",
        "\n",
        "\n",
        "                Based on the provided data:\n",
        "                1. Summarize the key highlights of the companys performance.\n",
        "                2. Identify challenges or risks mentioned.\n",
        "                3. Provide insights into future strategies or guidance.\n",
        "\n",
        "                Provide the output as:\n",
        "                1. Key Highlights:\n",
        "                  - [Performance overview]\n",
        "                  - [Growth areas]\n",
        "                2. Challenges/Risks:\n",
        "                  - [Key risks]\n",
        "                3. Future Strategies:\n",
        "                  - [Summary of plans or initiatives]\n",
        "\n",
        "                Metadata:\n",
        "                - Year: {year}\n",
        "                - Bank: {bank}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                <|assistant|>\"\"\",\n",
        "                input_variables=[\"context\", \"question\", \"year\", \"bank\"]\n",
        "            ),\n",
        "            \"single_quarter_sentiment\": PromptTemplate(\n",
        "                template=\"\"\"<|system|>\n",
        "                You are a senior investment analyst at a major hedge fund.\n",
        "                Determine and analyze sentiment for a single quarter.\n",
        "                Provide insights into the sentiment, including whether it is positive, negative, or neutral, and why.\n",
        "\n",
        "                Metadata:\n",
        "                - Year: {year}\n",
        "                - Quarter: {quarter}\n",
        "                - Bank: {bank}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                <|assistant|>\"\"\",\n",
        "                input_variables=[\"context\", \"question\", \"year\", \"quarter\", \"bank\"]\n",
        "            ),\n",
        "            \"year_sentiment_trends\": PromptTemplate(\n",
        "                template=\"\"\"<|system|>\n",
        "                You are a senior investment analyst at a major hedge fund.\n",
        "                Determine and analyze sentiment changes over a year.\n",
        "                Provide insights into the sentiment, including whether it is positive, negative, or neutral, and why.\n",
        "                Identify shifts in sentiment across quarters and provide possible reasons.\n",
        "\n",
        "                Metadata:\n",
        "                - Year: {year}\n",
        "                - Bank: {bank}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                <|assistant|>\"\"\",\n",
        "                input_variables=[\"context\", \"question\", \"year\", \"bank\"]\n",
        "            ),\n",
        "            \"summarize_single_quarter\": PromptTemplate(\n",
        "                template=\"\"\"<|system|>\n",
        "                You are a senior investment analyst at a major hedge fund.\n",
        "                Summarize the main points of a single quarter's earnings call data.\n",
        "                Include highlights and key performance indicators.\n",
        "\n",
        "                Metadata:\n",
        "                - Year: {year}\n",
        "                - Quarter: {quarter}\n",
        "                - Bank: {bank}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                <|assistant|>\"\"\",\n",
        "                input_variables=[\"context\", \"question\", \"year\", \"quarter\", \"bank\"]\n",
        "            ),\n",
        "            \"aggregated_summaries\": PromptTemplate(\n",
        "                template=\"\"\"<|system|>\n",
        "                You are a senior investment analyst at a major hedge fund.\n",
        "                Summarize the financial performance and sentiment.\n",
        "                Include key trends, highlights, and areas of concern.\n",
        "\n",
        "                Metadata:\n",
        "                {metadata}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                <|assistant|>\"\"\",\n",
        "                input_variables=[\"metadata\", \"context\", \"question\"]\n",
        "            ),\n",
        "            \"trend_analysis\": PromptTemplate(\n",
        "                template=\"\"\"<|system|>\n",
        "                You are a senior investment analyst at a major hedge fund.\n",
        "                Analyze trends in financial metrics over time.\n",
        "                Highlight significant changes and provide actionable insights.\n",
        "\n",
        "                Metadata:\n",
        "                {metadata}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                <|assistant|>\"\"\",\n",
        "                input_variables=[\"metadata\", \"context\", \"question\"]\n",
        "            ),\n",
        "        }\n",
        "\n",
        "        # Validate templates\n",
        "        for key, template in prompt_templates.items():\n",
        "            missing_vars = [var for var in template.input_variables if f\"{{{var}}}\" not in template.template]\n",
        "            unused_vars = [\n",
        "                var.split(\"}\")[0]\n",
        "                for var in template.template.split(\"{\")\n",
        "                if \"}\" in var and var.split(\"}\")[0] not in template.input_variables\n",
        "            ]\n",
        "\n",
        "            if missing_vars:\n",
        "                logger.warning(f\"Template '{key}' is missing variables: {missing_vars}\")\n",
        "            if unused_vars:\n",
        "                logger.warning(f\"Template '{key}' contains unused variables: {unused_vars}\")\n",
        "            if not missing_vars and not unused_vars:\n",
        "                logger.info(f\"Template '{key}' validated successfully.\")\n",
        "\n",
        "        # Validate chain_type\n",
        "        valid_chain_types = [\"stuff\", \"map_reduce\", \"refine\"]\n",
        "        if chain_type not in valid_chain_types:\n",
        "            raise ValueError(f\"Invalid chain_type '{chain_type}'. Valid options are: {valid_chain_types}\")\n",
        "\n",
        "        # Create chains dynamically\n",
        "        chains = {\n",
        "            key: load_qa_chain(llm, chain_type=chain_type, prompt=prompt)\n",
        "            for key, prompt in prompt_templates.items()\n",
        "        }\n",
        "\n",
        "        logger.info(\"QA chains initialized successfully.\")\n",
        "        return chains\n",
        "\n",
        "    except ValueError as ve:\n",
        "        logger.error(f\"Validation error: {ve}\")\n",
        "        raise RuntimeError(f\"Failed to initialize QA chains: {ve}\")\n",
        "    except Exception as e:\n",
        "        logger.exception(\"An unexpected error occurred while initializing QA chains.\")\n",
        "        raise RuntimeError(f\"Failed to initialize QA chains: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAkW8I3bHe30"
      },
      "outputs": [],
      "source": [
        "def get_filters():\n",
        "    filters = {}\n",
        "    filter_bank = input(\"Do you want to filter by a specific bank? (yes/no): \").strip().lower()\n",
        "    if filter_bank == \"yes\":\n",
        "        filters[\"bank\"] = input(\"Enter the bank name: \").strip().lower()\n",
        "    year = input(\"Enter the year to filter (or press Enter to skip): \").strip()\n",
        "    if year:\n",
        "        filters[\"year\"] = year\n",
        "    quarter = input(\"Enter the quarter to filter (e.g., Q1, or press Enter to skip): \").strip()\n",
        "    if quarter:\n",
        "        filters[\"quarter\"] = quarter\n",
        "    return filters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAxYdyz-He30"
      },
      "source": [
        "## Context Preparation\n",
        "\n",
        "Purpose: Functions to prepare text and metadata context dynamically based on token limits and requirements for answering queries.\n",
        "\n",
        "**Functions:**\n",
        "- prepare_context_with_dynamic_chunking\n",
        "- prepare_context_with_metadata\n",
        "- prepare_context_with_limit\n",
        "- filter_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1cT2fHxHe31"
      },
      "outputs": [],
      "source": [
        "def prepare_context_with_dynamic_chunking(\n",
        "    documents: List[Document],\n",
        "    summarization_pipeline: pipeline,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    model_token_limit: int,\n",
        "    prompt_tokens: int,\n",
        "    max_new_tokens: int,\n",
        "    bank: str\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Prepares the context by dynamically chunking documents and summarizing if necessary.\n",
        "\n",
        "    Args:\n",
        "        documents (List[Document]): List of relevant documents.\n",
        "        summarization_pipeline (pipeline): Summarization model pipeline.\n",
        "        tokenizer (AutoTokenizer): Tokenizer for token estimation.\n",
        "        model_token_limit (int): Maximum token limit for the model.\n",
        "        prompt_tokens (int): Number of tokens reserved for the prompt.\n",
        "        max_new_tokens (int): Number of tokens reserved for the model's response.\n",
        "        bank (str): The bank name to include in the context.\n",
        "\n",
        "    Returns:\n",
        "        str: The prepared context string.\n",
        "    \"\"\"\n",
        "    # Calculate the remaining token budget for context\n",
        "    context_token_budget = model_token_limit - prompt_tokens - max_new_tokens\n",
        "    logger.info(f\"Context token budget: {context_token_budget}\")\n",
        "\n",
        "    # Initialize context\n",
        "    context = \"\"\n",
        "    for doc in documents:\n",
        "        try:\n",
        "            # Tokenize the document content\n",
        "            tokens = tokenizer.encode(doc.page_content, truncation=False)\n",
        "            token_count = len(tokens)\n",
        "\n",
        "            # Check if the document fits within the budget\n",
        "            if token_count <= context_token_budget:\n",
        "                context += doc.page_content + \"\\n\"\n",
        "                context_token_budget -= token_count\n",
        "            else:\n",
        "                # Summarize if the document exceeds the token budget\n",
        "                logger.info(f\"Summarizing document with {token_count} tokens...\")\n",
        "                summary = summarization_pipeline(\n",
        "                    doc.page_content, max_length=200, min_length=50, do_sample=False\n",
        "                )\n",
        "                summary_text = summary[0][\"summary_text\"]\n",
        "                summary_tokens = tokenizer.encode(summary_text, truncation=False)\n",
        "\n",
        "                if len(summary_tokens) <= context_token_budget:\n",
        "                    context += summary_text + \"\\n\"\n",
        "                    context_token_budget -= len(summary_tokens)\n",
        "                else:\n",
        "                    logger.warning(\"Summarized content still exceeds token budget. Skipping this document.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing document for context: {e}\")\n",
        "            continue\n",
        "\n",
        "    return context.strip()\n",
        "\n",
        "def prepare_context_with_metadata(documents, metadata_df, filters):\n",
        "    filtered_metadata = metadata_df[\n",
        "        (metadata_df[\"Bank\"].str.lower() == filters[\"bank\"].lower()) &\n",
        "        (metadata_df[\"Year\"].str.lower() == filters[\"year\"].lower()) &\n",
        "        (metadata_df[\"Quarter\"].str.lower() == filters[\"quarter\"].lower())\n",
        "    ]\n",
        "    metadata_context = \"\\n\".join(\n",
        "        [f\"{row['Name']} ({row['Designation']}), {row['Bank']} {row['Quarter']} {row['Year']}\" for _, row in filtered_metadata.iterrows()]\n",
        "    )\n",
        "    return metadata_context\n",
        "\n",
        "\n",
        "def prepare_context_with_limit(\n",
        "    documents: List[Document],\n",
        "    tokenizer,\n",
        "    summarization_pipeline=None,\n",
        "    token_limit: int = 4000,\n",
        "    prompt_tokens: int = 200,\n",
        "    max_new_tokens: int = 500\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Prepares the context for the LLM by including entire documents or summaries if they exceed the token limit.\n",
        "\n",
        "    Args:\n",
        "        documents (List[Document]): List of Document objects.\n",
        "        tokenizer: Tokenizer for token estimation.\n",
        "        summarization_pipeline: Optional summarization pipeline for condensing long documents.\n",
        "        token_limit (int): Maximum token limit for the model.\n",
        "        prompt_tokens (int): Estimated token count for the prompt.\n",
        "        max_new_tokens (int): Estimated token count for the model's response.\n",
        "\n",
        "    Returns:\n",
        "        str: Prepared context string within the token budget.\n",
        "    \"\"\"\n",
        "    # Calculate the context token budget\n",
        "    context_token_limit = token_limit - prompt_tokens - max_new_tokens\n",
        "    logger.info(f\"Context token budget: {context_token_limit} tokens.\")\n",
        "\n",
        "    context = \"\"\n",
        "\n",
        "    for doc in documents:\n",
        "        try:\n",
        "            # Calculate token count for the document content\n",
        "            doc_tokens = len(tokenizer.encode(doc.page_content, truncation=False))\n",
        "            logger.info(f\"Document token count: {doc_tokens}\")\n",
        "\n",
        "            # If the document fits within the token budget, add it directly\n",
        "            if doc_tokens <= context_token_limit:\n",
        "                context += doc.page_content + \"\\n\"\n",
        "                context_token_limit -= doc_tokens\n",
        "            else:\n",
        "                # Summarize if the document exceeds the token limit and summarization is available\n",
        "                if summarization_pipeline:\n",
        "                    logger.info(f\"Summarizing document with {doc_tokens} tokens...\")\n",
        "                    summary = summarization_pipeline(\n",
        "                        doc.page_content, max_length=200, min_length=50, do_sample=False\n",
        "                    )\n",
        "                    summary_text = summary[0][\"summary_text\"]\n",
        "                    summary_tokens = len(tokenizer.encode(summary_text, truncation=False))\n",
        "\n",
        "                    if summary_tokens <= context_token_limit:\n",
        "                        context += summary_text + \"\\n\"\n",
        "                        context_token_limit -= summary_tokens\n",
        "                    else:\n",
        "                        logger.warning(\"Summarized content still exceeds token limit. Skipping document.\")\n",
        "                else:\n",
        "                    logger.warning(\"No summarization pipeline provided. Skipping document.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing document for context: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "    return context.strip()\n",
        "\n",
        "def prepare_context_with_topics_and_sentiments(\n",
        "    documents: List[Document],\n",
        "    tokenizer,\n",
        "    summarization_pipeline=None,\n",
        "    sentiment_pipeline=None,\n",
        "    lda_model=None,\n",
        "    dictionary=None,\n",
        "    bow_corpus=None,\n",
        "    num_topics=3,\n",
        "    token_limit: int = 4000,\n",
        "    prompt_tokens: int = 200,\n",
        "    max_new_tokens: int = 500\n",
        "):\n",
        "    \"\"\"\n",
        "    This function is for future use to integrate LDA and sentiment analysis.\n",
        "    Prepares the context for the LLM by including document content, summaries, LDA topics, and sentiment analysis\n",
        "\n",
        "    Args:\n",
        "        documents (List[Document]): List of Document objects.\n",
        "        tokenizer: Tokenizer for token estimation.\n",
        "        summarization_pipeline: Optional summarization pipeline for condensing long documents.\n",
        "        sentiment_pipeline: Hugging Face sentiment-analysis pipeline.\n",
        "        lda_model (LdaModel): Trained LDA model for topic extraction.\n",
        "        dictionary (Dictionary): Gensim dictionary for the corpus.\n",
        "        bow_corpus (list): Bag-of-words representation of the corpus.\n",
        "        num_topics (int): Number of topics to extract for each document.\n",
        "        token_limit (int): Maximum token limit for the model.\n",
        "        prompt_tokens (int): Estimated token count for the prompt.\n",
        "        max_new_tokens (int): Estimated token count for the model's response.\n",
        "\n",
        "    Returns:\n",
        "        str: Prepared context string within the token budget.\n",
        "    \"\"\"\n",
        "    context_token_limit = token_limit - prompt_tokens - max_new_tokens\n",
        "    logger.info(f\"Context token budget: {context_token_limit} tokens.\")\n",
        "\n",
        "    context = \"\"\n",
        "    topic_context = \"\"\n",
        "    sentiment_context = \"\"\n",
        "\n",
        "    # Step 1: Extract topics using LDA\n",
        "    if lda_model and dictionary and bow_corpus:\n",
        "        logger.info(\"Extracting topics from LDA model...\")\n",
        "        topics_for_documents = extract_topics_from_lda(lda_model, bow_corpus, dictionary, num_topics)\n",
        "        topic_context = generate_topic_context(topics_for_documents)\n",
        "\n",
        "    # Step 2: Perform sentiment analysis\n",
        "    if sentiment_pipeline:\n",
        "        logger.info(\"Analyzing sentiments...\")\n",
        "        sentiments = []\n",
        "        for doc in documents:\n",
        "            try:\n",
        "                sentiment_result = sentiment_pipeline(doc.page_content)\n",
        "                sentiment_label = sentiment_result[0]['label']\n",
        "                sentiment_score = sentiment_result[0]['score']\n",
        "                sentiments.append(f\"{doc.metadata.get('source', 'Unknown')}: {sentiment_label} (Confidence: {sentiment_score:.2f})\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error during sentiment analysis for document: {e}\")\n",
        "        sentiment_context = \"\\n\".join(sentiments)\n",
        "\n",
        "    # Step 3: Add document content or summaries\n",
        "    for doc in documents:\n",
        "        try:\n",
        "            # Tokenize document content\n",
        "            doc_tokens = len(tokenizer.encode(doc.page_content, truncation=False))\n",
        "            logger.info(f\"Document token count: {doc_tokens}\")\n",
        "\n",
        "            # Add document content if within token budget\n",
        "            if doc_tokens <= context_token_limit:\n",
        "                context += doc.page_content + \"\\n\"\n",
        "                context_token_limit -= doc_tokens\n",
        "            else:\n",
        "                # Summarize if content exceeds token budget\n",
        "                if summarization_pipeline:\n",
        "                    logger.info(f\"Summarizing document with {doc_tokens} tokens...\")\n",
        "                    summary = summarization_pipeline(\n",
        "                        doc.page_content, max_length=200, min_length=50, do_sample=False\n",
        "                    )\n",
        "                    summary_text = summary[0][\"summary_text\"]\n",
        "                    summary_tokens = len(tokenizer.encode(summary_text, truncation=False))\n",
        "\n",
        "                    if summary_tokens <= context_token_limit:\n",
        "                        context += summary_text + \"\\n\"\n",
        "                        context_token_limit -= summary_tokens\n",
        "                    else:\n",
        "                        logger.warning(\"Summarized content still exceeds token limit. Skipping document.\")\n",
        "                else:\n",
        "                    logger.warning(\"No summarization pipeline provided. Skipping document.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing document for context: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Step 4: Append topic and sentiment context if space allows\n",
        "    topic_tokens = len(tokenizer.encode(topic_context, truncation=False))\n",
        "    if topic_tokens <= context_token_limit:\n",
        "        context += \"\\n\" + topic_context\n",
        "        context_token_limit -= topic_tokens\n",
        "    else:\n",
        "        logger.warning(\"Topic context exceeds token limit. Skipping topic context.\")\n",
        "\n",
        "    sentiment_tokens = len(tokenizer.encode(sentiment_context, truncation=False))\n",
        "    if sentiment_tokens <= context_token_limit:\n",
        "        context += \"\\n\" + sentiment_context\n",
        "    else:\n",
        "        logger.warning(\"Sentiment context exceeds token limit. Skipping sentiment context.\")\n",
        "\n",
        "    return context.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter function\n",
        "\n",
        "This function filters number of documents based on metadata to prepare the context for the query."
      ],
      "metadata": {
        "id": "Q64tNkjYE3sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def filter_documents(documents: List[Document], bank: str = None, year: str = None, quarter: str = None, designation: str = None) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Filters documents by bank, year, quarter, and optionally designation, ensuring metadata keys and values are normalized.\n",
        "\n",
        "    Args:\n",
        "        documents (List[Document]): List of Document objects with metadata.\n",
        "        bank (str, optional): Bank name to filter by.\n",
        "        year (str, optional): Year to filter by.\n",
        "        quarter (str, optional): Quarter to filter by (e.g., Q1, Q2).\n",
        "        designation (str, optional): Designation to filter by (e.g., CFO). Default is None.\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: Filtered list of documents.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.info(f\"Starting filtering with criteria bank: {bank}, year: {year}, quarter: {quarter}, designation: {designation}\")\n",
        "\n",
        "        filtered_docs = []\n",
        "        for doc in documents:\n",
        "            # Normalize metadata keys and values\n",
        "            metadata = {key.lower(): str(value).strip().lower() for key, value in doc.metadata.items()}\n",
        "            logger.debug(f\"Document Metadata: {metadata}\")\n",
        "\n",
        "            # Extract metadata fields for filtering\n",
        "            doc_bank = metadata.get(\"bank\", \"\")\n",
        "            doc_year = metadata.get(\"year\", \"\")\n",
        "            doc_quarter = metadata.get(\"quarter\", \"\")\n",
        "            doc_designation = metadata.get(\"designation\", \"\")\n",
        "\n",
        "            # Apply strict matching criteria\n",
        "            bank_match = not bank or doc_bank == bank.strip().lower()\n",
        "            year_match = not year or doc_year == str(year).strip().lower()\n",
        "            quarter_match = not quarter or doc_quarter == quarter.strip().lower()\n",
        "            designation_match = not designation or doc_designation == designation.strip().lower()\n",
        "\n",
        "            # Log the match results for debugging\n",
        "            logger.debug(f\"Bank Match: {bank_match}, Year Match: {year_match}, Quarter Match: {quarter_match}, Designation Match: {designation_match}\")\n",
        "\n",
        "            if bank_match and year_match and quarter_match and designation_match:\n",
        "                filtered_docs.append(doc)\n",
        "\n",
        "        logger.info(f\"Filtered {len(filtered_docs)} documents for Bank: {bank}, Year: {year}, Quarter: {quarter}, Designation: {designation}\")\n",
        "        return filtered_docs\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error filtering documents: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "GhMq9sV7E24W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwjASjqtHe31"
      },
      "source": [
        "## RAG Query Handling\n",
        "\n",
        "Purpose: These functions handle user queries by retrieving relevant documents, preparing context, and invoking the appropriate QA chain.\n",
        "\n",
        "**Functions:**\n",
        "\n",
        "- handle_generic_query\n",
        "- handle_compare_quarters_same_year\n",
        "- handle_compare_quarters_diff_years\n",
        "- handle_year_comparison\n",
        "- handle_sentiment_single_quarter\n",
        "- handle_summarize_single_quarter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QawBBzPZHe32"
      },
      "outputs": [],
      "source": [
        "def handle_generic_query(pipeline, tokenizer, summarization_pipeline, model_token_limit, filters, query):\n",
        "    \"\"\"\n",
        "    Handles generic queries with optional filters and retrieves relevant documents.\n",
        "    \"\"\"\n",
        "    logger.info(\"Handling a generic query.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Apply filters if provided\n",
        "        logger.info(\"Retrieving relevant documents with filters...\")\n",
        "        relevant_docs = pipeline[\"retriever\"].get_relevant_documents(query)\n",
        "        if filters.get(\"bank\"):\n",
        "            relevant_docs = [\n",
        "                doc for doc in relevant_docs if doc.metadata.get(\"bank\", \"\").lower() == filters[\"bank\"].lower()\n",
        "            ]\n",
        "\n",
        "        if not relevant_docs:\n",
        "            logger.warning(\"No documents found matching the query criteria.\")\n",
        "            return \"[ERROR] No documents found for the query.\"\n",
        "\n",
        "        logger.info(f\"Filtered {len(relevant_docs)} documents for the generic query.\")\n",
        "\n",
        "        # Step 2: Prepare context\n",
        "        logger.info(\"Preparing context for the query...\")\n",
        "        context = prepare_context_with_limit(\n",
        "            documents=relevant_docs,\n",
        "            tokenizer=tokenizer,\n",
        "            summarization_pipeline=summarization_pipeline,\n",
        "            token_limit=model_token_limit,\n",
        "            prompt_tokens=200,\n",
        "            max_new_tokens=500,\n",
        "        )\n",
        "\n",
        "        if not context.strip():\n",
        "            logger.error(\"No relevant context could be prepared within the token limit.\")\n",
        "            return \"[ERROR] Unable to prepare context.\"\n",
        "\n",
        "        # Step 3: Ensure relevant_docs are in Document format\n",
        "        input_documents = [\n",
        "            doc if isinstance(doc, Document) else Document(page_content=doc[\"page_content\"], metadata=doc[\"metadata\"])\n",
        "            for doc in relevant_docs\n",
        "        ]\n",
        "\n",
        "        # Step 4: Invoke the QA chain\n",
        "        logger.info(\"Invoking the QA chain...\")\n",
        "        response = pipeline[\"qa_chains\"][\"generic_question\"].invoke(\n",
        "            {\n",
        "                \"input_documents\": input_documents,\n",
        "                \"metadata\": f\"Filters: {filters}\",\n",
        "                \"context\": context,\n",
        "                \"question\": query,\n",
        "            },\n",
        "            repetition_penalty=1.2,\n",
        "            no_repeat_ngram_size=3,\n",
        "        )\n",
        "\n",
        "        # Extract and return the response\n",
        "        return extract_answer(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error handling the generic query: {e}\")\n",
        "        return \"[ERROR] An unexpected error occurred while handling the query.\"\n",
        "\n",
        "\n",
        "def handle_compare_quarters_same_year(pipeline, tokenizer, model_token_limit, year, quarter1, quarter2, bank, query):\n",
        "    logger.info(f\"Comparing {quarter1} and {quarter2} in {year} for {bank}.\")\n",
        "    try:\n",
        "        # Retrieve documents for Q1\n",
        "        filters_q1 = {\"year\": year, \"quarter\": quarter1, \"bank\": bank.lower()}\n",
        "        relevant_docs_q1 = pipeline[\"retriever\"].get_relevant_documents(query, filters=filters_q1)\n",
        "\n",
        "        # Retrieve documents for Q2\n",
        "        filters_q2 = {\"year\": year, \"quarter\": quarter2, \"bank\": bank.lower()}\n",
        "        relevant_docs_q2 = pipeline[\"retriever\"].get_relevant_documents(query, filters=filters_q2)\n",
        "\n",
        "        # Combine documents\n",
        "        input_documents = relevant_docs_q1 + relevant_docs_q2\n",
        "\n",
        "        if not input_documents:\n",
        "            logger.warning(\"No relevant documents found for the comparison.\")\n",
        "            return \"[ERROR] No relevant documents found for the comparison.\"\n",
        "\n",
        "        # Prepare metadata and invoke QA chain\n",
        "        metadata = f\"Year: {year}, Bank: {bank}, Quarter 1: {quarter1}, Quarter 2: {quarter2}\"\n",
        "        response = pipeline[\"qa_chains\"][\"compare_two_quarters_same_year\"].invoke(\n",
        "            {\n",
        "                \"input_documents\": input_documents,\n",
        "                \"metadata\": metadata,\n",
        "                \"context\": \"N/A\",\n",
        "                \"question\": query,\n",
        "                \"year\": year,\n",
        "                \"quarter1\": quarter1,\n",
        "                \"quarter2\": quarter2,\n",
        "                \"bank\": bank,\n",
        "            }\n",
        "        )\n",
        "        return extract_answer(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error comparing two quarters in the same year: {e}\")\n",
        "        return \"[ERROR] An unexpected error occurred.\"\n",
        "\n",
        "\n",
        "\n",
        "def handle_compare_quarters_diff_years(pipeline, tokenizer, model_token_limit, year1, year2, quarter1, quarter2, bank, query):\n",
        "    \"\"\"\n",
        "    Handles comparison of two quarters across different years.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Comparing {quarter1} ({year1}) and {quarter2} ({year2}) for {bank}.\")\n",
        "\n",
        "    try:\n",
        "        # Retrieve documents for Quarter 1 of Year 1\n",
        "        filters = {\"year\": year1, \"quarter\": quarter1, \"bank\": bank.lower()}\n",
        "        logger.info(f\"Retrieving documents for {quarter1} {year1}...\")\n",
        "        relevant_docs_q1 = pipeline[\"retriever\"].get_relevant_documents(query, filters=filters)\n",
        "\n",
        "        # Retrieve documents for Quarter 2 of Year 2\n",
        "        filters[\"year\"] = year2\n",
        "        filters[\"quarter\"] = quarter2\n",
        "        logger.info(f\"Retrieving documents for {quarter2} {year2}...\")\n",
        "        relevant_docs_q2 = pipeline[\"retriever\"].get_relevant_documents(query, filters=filters)\n",
        "\n",
        "        # Combine documents from both quarters\n",
        "        input_documents = relevant_docs_q1 + relevant_docs_q2\n",
        "\n",
        "        if not input_documents:\n",
        "            logger.warning(\"No relevant documents found for the comparison.\")\n",
        "            return \"[ERROR] No relevant documents found for the comparison.\"\n",
        "\n",
        "        # Prepare metadata\n",
        "        metadata = f\"Bank: {bank}, Quarter 1: {quarter1} ({year1}), Quarter 2: {quarter2} ({year2})\"\n",
        "\n",
        "        # Invoke the QA chain\n",
        "        response = pipeline[\"qa_chains\"][\"compare_two_quarters_diff_years\"].invoke(\n",
        "            {\n",
        "                \"input_documents\": input_documents,\n",
        "                \"metadata\": metadata,\n",
        "                \"context\": \"N/A\",\n",
        "                \"question\": query,\n",
        "                \"year1\": year1,\n",
        "                \"year2\": year2,\n",
        "                \"quarter1\": quarter1,\n",
        "                \"quarter2\": quarter2,\n",
        "                \"bank\": bank,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Extract and return the response\n",
        "        return extract_answer(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error comparing two quarters across different years: {e}\")\n",
        "        return \"[ERROR] An unexpected error occurred.\"\n",
        "\n",
        "\n",
        "\n",
        "def handle_year_comparison(pipeline, tokenizer, model_token_limit, year1, year2, bank, query):\n",
        "    \"\"\"\n",
        "    Handles year-over-year comparison.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Comparing performance for {year1} and {year2} for {bank}.\")\n",
        "\n",
        "    try:\n",
        "        # Retrieve documents for Year 1\n",
        "        filters = {\"year\": year1, \"bank\": bank.lower()}\n",
        "        logger.info(f\"Retrieving documents for {year1}...\")\n",
        "        relevant_docs_y1 = pipeline[\"retriever\"].get_relevant_documents(query, filters=filters)\n",
        "\n",
        "        # Retrieve documents for Year 2\n",
        "        filters[\"year\"] = year2\n",
        "        logger.info(f\"Retrieving documents for {year2}...\")\n",
        "        relevant_docs_y2 = pipeline[\"retriever\"].get_relevant_documents(query, filters=filters)\n",
        "\n",
        "        # Combine documents from both years\n",
        "        input_documents = relevant_docs_y1 + relevant_docs_y2\n",
        "\n",
        "        if not input_documents:\n",
        "            logger.warning(\"No relevant documents found for the comparison.\")\n",
        "            return \"[ERROR] No relevant documents found for the comparison.\"\n",
        "\n",
        "        # Prepare metadata\n",
        "        metadata = f\"Year 1: {year1}, Year 2: {year2}, Bank: {bank}\"\n",
        "\n",
        "        # Invoke the QA chain\n",
        "        response = pipeline[\"qa_chains\"][\"year_comparison\"].invoke(\n",
        "            {\n",
        "                \"input_documents\": input_documents,\n",
        "                \"metadata\": metadata,\n",
        "                \"context\": \"N/A\",\n",
        "                \"question\": query,\n",
        "                \"year1\": year1,\n",
        "                \"year2\": year2,\n",
        "                \"bank\": bank,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Extract and return the response\n",
        "        return extract_answer(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error performing year-over-year comparison: {e}\")\n",
        "        return \"[ERROR] An unexpected error occurred.\"\n",
        "\n",
        "\n",
        "def handle_sentiment_single_quarter(pipeline, tokenizer, model_token_limit, year, quarter, bank, query):\n",
        "    \"\"\"\n",
        "    Handles sentiment analysis for a single quarter.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Analyzing sentiment for {bank} in {quarter} {year}.\")\n",
        "\n",
        "    try:\n",
        "        # Retrieve documents for the given quarter\n",
        "        filters = {\"year\": year, \"quarter\": quarter, \"bank\": bank.lower()}\n",
        "        logger.info(f\"Retrieving documents for {quarter} {year}...\")\n",
        "        input_documents = pipeline[\"retriever\"].get_relevant_documents(query, filters=filters)\n",
        "\n",
        "        if not input_documents:\n",
        "            logger.warning(\"No relevant documents found for sentiment analysis.\")\n",
        "            return \"[ERROR] No relevant documents found for sentiment analysis.\"\n",
        "\n",
        "        # Prepare metadata\n",
        "        metadata = f\"Year: {year}, Quarter: {quarter}, Bank: {bank}\"\n",
        "\n",
        "        # Invoke the QA chain\n",
        "        response = pipeline[\"qa_chains\"][\"single_quarter_sentiment\"].invoke(\n",
        "            {\n",
        "                \"input_documents\": input_documents,\n",
        "                \"metadata\": metadata,\n",
        "                \"context\": \"N/A\",\n",
        "                \"question\": query,\n",
        "                \"year\": year,\n",
        "                \"quarter\": quarter,\n",
        "                \"bank\": bank,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Extract and return the response\n",
        "        return extract_answer(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error analyzing sentiment for a single quarter: {e}\")\n",
        "        return \"[ERROR] An unexpected error occurred.\"\n",
        "\n",
        "\n",
        "def handle_summarize_single_quarter(pipeline, tokenizer, model_token_limit, year, quarter, bank, query):\n",
        "    \"\"\"\n",
        "    Handles summarization for a single quarter.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Summarizing performance for {bank} in {quarter} {year}.\")\n",
        "\n",
        "    try:\n",
        "        # Retrieve documents for the given quarter\n",
        "        filters = {\"year\": year, \"quarter\": quarter, \"bank\": bank.lower()}\n",
        "        logger.info(f\"Retrieving documents for {quarter} {year}...\")\n",
        "        input_documents = pipeline[\"retriever\"].get_relevant_documents(query, filters=filters)\n",
        "\n",
        "        if not input_documents:\n",
        "            logger.warning(\"No relevant documents found for summarization.\")\n",
        "            return \"[ERROR] No relevant documents found for summarization.\"\n",
        "\n",
        "        # Prepare metadata\n",
        "        metadata = f\"Year: {year}, Quarter: {quarter}, Bank: {bank}\"\n",
        "\n",
        "        # Invoke the QA chain\n",
        "        response = pipeline[\"qa_chains\"][\"summarize_single_quarter\"].invoke(\n",
        "            {\n",
        "                \"input_documents\": input_documents,\n",
        "                \"metadata\": metadata,\n",
        "                \"context\": \"N/A\",\n",
        "                \"question\": query,\n",
        "                \"year\": year,\n",
        "                \"quarter\": quarter,\n",
        "                \"bank\": bank,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Extract and return the response\n",
        "        return extract_answer(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error summarizing the single quarter: {e}\")\n",
        "        return \"[ERROR] An unexpected error occurred while summarizing the quarter.\"\n",
        "\n",
        "\n",
        "# ==============================Topic Modeling=======================================\n",
        "\n",
        "def handle_bertopic(documents, nlp_model=None):\n",
        "    \"\"\"\n",
        "    Perform BERTopic modeling on earnings call transcripts.\n",
        "\n",
        "    Args:\n",
        "        documents (List[Document]): A list of filtered documents to perform topic modeling on.\n",
        "        nlp_model (spacy.Language): Spacy NER model for removing names (PERSON entities).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting BERTopic modeling for earnings call transcripts...\")\n",
        "\n",
        "    try:\n",
        "        # Combine all document content into a single corpus\n",
        "        corpus = [doc.page_content for doc in documents if doc.page_content]\n",
        "\n",
        "        if not corpus:\n",
        "            logger.warning(\"No valid content found in documents for BERTopic.\")\n",
        "            print(\"[ERROR] No valid content to analyze with BERTopic.\")\n",
        "            return\n",
        "\n",
        "        # Preprocess the corpus for earnings call transcripts\n",
        "        logger.info(\"Preprocessing the corpus for earnings calls...\")\n",
        "        redundant_terms = [\n",
        "            \"credit suisse\", \"q1\", \"q2\", \"q3\", \"q4\",\n",
        "            \"first quarter\", \"second quarter\", \"third quarter\", \"fourth quarter\",\n",
        "            \"earnings call\", \"company\", \"presentation\", \"analyst\", \"operator\", \"seeking_alpha\"\n",
        "        ]\n",
        "        cleaned_corpus = preprocess_corpus_bertopic(\n",
        "            corpus, nlp_model, custom_stopwords=redundant_terms, redundant_terms=redundant_terms\n",
        "        )\n",
        "\n",
        "        # Custom vectorizer for bigrams and trigrams with stopwords\n",
        "        vectorizer = CountVectorizer(ngram_range=(2, 3), stop_words=\"english\")\n",
        "\n",
        "        # Initialize BERTopic model with custom vectorizer\n",
        "        topic_model = BERTopic(vectorizer_model=vectorizer)\n",
        "\n",
        "        # Fit the model on the cleaned corpus\n",
        "        logger.info(\"Fitting BERTopic model to the cleaned corpus...\")\n",
        "        topics, probs = topic_model.fit_transform(cleaned_corpus)\n",
        "\n",
        "        # Check if any topics were identified\n",
        "        topic_info = topic_model.get_topic_info()\n",
        "        if topic_info.empty or topic_info[\"Count\"].sum() == 0:\n",
        "            logger.warning(\"No meaningful topics were identified by BERTopic.\")\n",
        "            print(\"[ERROR] BERTopic did not identify any meaningful topics. Check the input data.\")\n",
        "            return\n",
        "\n",
        "        # Diagnostics: Display document lengths and topic info\n",
        "        logger.info(\"Diagnostics:\")\n",
        "        doc_lengths = [len(doc.split()) for doc in cleaned_corpus]\n",
        "        logger.info(f\"Average document length: {sum(doc_lengths) / len(doc_lengths):.2f}\")\n",
        "        logger.info(f\"Total number of documents: {len(cleaned_corpus)}\")\n",
        "        logger.info(f\"Topic Info: \\n{topic_info.head(6)}\")\n",
        "\n",
        "        # Display top topics\n",
        "        logger.info(\"Displaying top topics for earnings calls...\")\n",
        "        print(\"\\nTop Topics Identified by BERTopic:\")\n",
        "        print(topic_info.head(6))  # Display the top 6 topics for readability\n",
        "\n",
        "        # Try visualizing topics if valid topics exist\n",
        "        try:\n",
        "            fig = topic_model.visualize_barchart(top_n_topics=6)\n",
        "            fig.show()\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Visualization failed: {e}\")\n",
        "            print(\"[WARNING] Unable to generate visualization. Skipping...\")\n",
        "\n",
        "        # Save the model for later use\n",
        "        topic_model.save(\"bertopic_model_earnings_calls\")\n",
        "        logger.info(\"BERTopic model saved successfully.\")\n",
        "\n",
        "        print(\"[INFO] BERTopic modeling for earnings calls completed successfully.\")\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error during BERTopic modeling: {e}\")\n",
        "        print(f\"[ERROR] An error occurred during BERTopic modeling: {e}\")\n",
        "\n",
        "\n",
        "def handle_gensim_lda(documents, nlp_model=None, num_topics=3, extra_redundant_terms=None):\n",
        "    \"\"\"\n",
        "    Perform LDA topic modeling on earnings call transcripts using Gensim, focusing on bigrams and trigrams.\n",
        "\n",
        "    Args:\n",
        "        documents (List[Document]): A list of filtered documents with a `page_content` attribute.\n",
        "        nlp_model (spacy.Language): Spacy NER model for removing names (PERSON entities).\n",
        "        num_topics (int): Number of topics to extract.\n",
        "        extra_redundant_terms (list): Additional terms to remove during preprocessing.\n",
        "\n",
        "    Returns:\n",
        "        LdaModel, Dictionary, List: Trained LDA model, dictionary, and bow_corpus.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Gensim LDA modeling for earnings call transcripts...\")\n",
        "\n",
        "    try:\n",
        "        # Combine all document content into a single corpus\n",
        "        corpus = [doc.page_content for doc in documents if doc.page_content]\n",
        "\n",
        "        if not corpus:\n",
        "            logger.warning(\"No valid content found in documents for Gensim LDA.\")\n",
        "            print(\"[ERROR] No valid content to analyze with Gensim LDA.\")\n",
        "            return\n",
        "\n",
        "        # Preprocess the corpus for earnings call transcripts\n",
        "        logger.info(\"Preprocessing the corpus for earnings calls...\")\n",
        "        redundant_terms = [\n",
        "            \"credit suisse\", \"q1\", \"q2\", \"q3\", \"q4\",\n",
        "            \"first quarter\", \"second quarter\", \"third quarter\", \"fourth quarter\",\n",
        "            \"earnings call\", \"company\", \"presentation\", \"analyst\", \"operator\", \"thomas\", \"gottstein\", \"transcript_seeking alpha\", \"question\", \"next slide\", \"thank you\",\n",
        "            \"take question\", \"transcript seeking alpha\", \"group ag cs results\", \"morning\", \"Transcript _ Seeking Alpha\", \"Group AG\", \"Results\"\n",
        "        ]\n",
        "\n",
        "        # Add any extra redundant terms\n",
        "        if extra_redundant_terms:\n",
        "            redundant_terms.extend(extra_redundant_terms)\n",
        "\n",
        "        processed_corpus = preprocess_corpus_gensim(\n",
        "            corpus,\n",
        "            nlp_model,\n",
        "            custom_stopwords=redundant_terms,\n",
        "            redundant_terms=redundant_terms,\n",
        "        )\n",
        "\n",
        "        # Create a dictionary and a bag-of-words representation of the corpus\n",
        "        logger.info(\"Creating dictionary and bag-of-words representation...\")\n",
        "        dictionary = Dictionary(processed_corpus)\n",
        "        bow_corpus = [dictionary.doc2bow(doc) for doc in processed_corpus]\n",
        "\n",
        "        # Log dictionary size\n",
        "        logger.info(f\"Vocabulary size after preprocessing: {len(dictionary)}\")\n",
        "\n",
        "        # Train LDA model\n",
        "        logger.info(f\"Training Gensim LDA model with {num_topics} topics...\")\n",
        "        lda_model = LdaModel(\n",
        "            corpus=bow_corpus,\n",
        "            id2word=dictionary,\n",
        "            num_topics=num_topics,\n",
        "            random_state=42,\n",
        "            passes=10,\n",
        "            iterations=50,\n",
        "        )\n",
        "\n",
        "        # Display topics\n",
        "        logger.info(\"Displaying top topics for earnings calls...\")\n",
        "        print(\"\\nTop Topics Identified by Gensim LDA:\")\n",
        "        for idx, topic in lda_model.print_topics(num_topics=num_topics, num_words=10):\n",
        "            print(f\"Topic #{idx + 1}: {topic}\")\n",
        "\n",
        "        # Visualize topics in Colab Notebook\n",
        "        try:\n",
        "            logger.info(\"Preparing visualization for LDA topics...\")\n",
        "            vis_data = gensimvis.prepare(lda_model, bow_corpus, dictionary)\n",
        "\n",
        "            # Check if running in a Colab or IPython environment\n",
        "            from IPython import get_ipython\n",
        "            if \"google.colab\" in str(get_ipython()):  # Specifically for Google Colab\n",
        "                from google.colab import output\n",
        "                pyLDAvis.enable_notebook()\n",
        "                display(vis_data)\n",
        "            elif \"IPython.core.interactiveshell\" in str(type(get_ipython())):  # Jupyter notebook\n",
        "                pyLDAvis.enable_notebook()\n",
        "                display(vis_data)\n",
        "            else:\n",
        "                logger.warning(\"Visualization not supported in this environment. Saving as HTML.\")\n",
        "                pyLDAvis.save_html(vis_data, \"gensim_lda_topics.html\")\n",
        "\n",
        "            logger.info(\"LDA visualization successfully displayed or saved.\")\n",
        "        except ImportError as e:\n",
        "            logger.warning(f\"pyLDAvis not installed or visualization failed: {e}\")\n",
        "            print(\"[WARNING] Unable to generate visualization. Skipping...\")\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"Unexpected error during visualization: {e}\")\n",
        "            print(\"[ERROR] Unable to display visualization.\")\n",
        "\n",
        "        print(\"[INFO] Gensim LDA modeling for earnings calls completed successfully.\")\n",
        "        return lda_model, dictionary, bow_corpus\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error during Gensim LDA modeling: {e}\")\n",
        "        print(f\"[ERROR] An error occurred during Gensim LDA modeling: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display Functions"
      ],
      "metadata": {
        "id": "g7Hdy1qDF_Z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def display_wrapped_output(output_text, width=50):\n",
        "    \"\"\"\n",
        "    Displays the output text wrapped to fit within the specified width.\n",
        "\n",
        "    Args:\n",
        "        output_text (str): The text to display.\n",
        "        width (int): The maximum width of each line.\n",
        "    \"\"\"\n",
        "    wrapper = textwrap.TextWrapper(width=width)\n",
        "    wrapped_text = wrapper.fill(output_text)\n",
        "    print(\"[RESULT] Response from the AI Assistant:\")\n",
        "    print(wrapped_text)\n",
        "\n",
        "def display_title():\n",
        "    print(\"\\n\\n\" + \"=\" * 70)\n",
        "    print(\"   Welcome to the Earning Call Transcript-Based Risk Analyzer\")\n",
        "    print(\"             An Early Warning System for Investors\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "\n",
        "def display_query_menu():\n",
        "    \"\"\"\n",
        "    Displays a menu for selecting the query type and returns the user's choice.\n",
        "\n",
        "    Returns:\n",
        "        int: The selected numeric choice, or None if the input is invalid.\n",
        "    \"\"\"\n",
        "    # Define the menu options\n",
        "    menu_options = {\n",
        "        \"1\": \"Ask a generic question\",\n",
        "        \"2\": \"Compare two quarters of the same year\",\n",
        "        \"3\": \"Compare two quarters of different years\",\n",
        "        \"4\": \"Year-over-year comparison\",\n",
        "        \"5\": \"Analyze all quarters of a year\",\n",
        "        \"6\": \"Analyze sentiment for a single quarter\",\n",
        "        \"7\": \"Summarize a single quarter\",\n",
        "        \"8\": \"Perform Topic Modeling with BERTopic\",\n",
        "        \"9\": \"Perform Topic Modeling with Gensim LDA\",\n",
        "        \"10\": \"Exit\"\n",
        "    }\n",
        "\n",
        "    # Display the menu\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"   Query Selection Menu\")\n",
        "    print(\"=\" * 50)\n",
        "    for key, value in menu_options.items():\n",
        "        print(f\"{key}. {value}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Get the user's choice\n",
        "    choice = input(\"Select an option (1-10): \").strip()\n",
        "\n",
        "    # Validate the choice and return as an integer\n",
        "    if choice in menu_options:\n",
        "        return int(choice)\n",
        "    else:\n",
        "        print(\"[ERROR] Invalid choice. Please enter a number between 1 and 10.\")\n",
        "        return None\n",
        "\n",
        "def remove_repetitions(text):\n",
        "    sentences = text.split('. ')\n",
        "    seen = set()\n",
        "    unique_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if sentence not in seen:\n",
        "            unique_sentences.append(sentence)\n",
        "            seen.add(sentence)\n",
        "    return '. '.join(unique_sentences)"
      ],
      "metadata": {
        "id": "jy8NIeTRJLFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 'main' function\n",
        "\n",
        "**Flow of main function**\n",
        "\n",
        "1. **Initialize Logging**\n",
        "   - Start the logger to track the application's workflow.\n",
        "\n",
        "2. **Initialize Pipeline Configuration**\n",
        "   - **Set File Paths**: Specify paths for document folder and metadata CSV.\n",
        "   - **Select Model**: Prompt user to select a language model.\n",
        "   - **Set Token Limits**: Retrieve the token limit for the selected model.\n",
        "   - **Configure Pipeline**: Call `configure_rag_pipeline` to set up the RAG pipeline with:\n",
        "     - Document preprocessing\n",
        "     - Embedding initialization\n",
        "     - Vector store setup\n",
        "     - Summarization and sentiment analysis pipelines\n",
        "     - LDA topic modeling\n",
        "   - **Setup Retriever**: Add a retriever to the pipeline for retrieving relevant documents.\n",
        "   - **Load Tokenizer**: Initialize tokenizer for the selected model.\n",
        "\n",
        "3. **Define Helper Functions**\n",
        "   - **`get_filters`**: Collect filter criteria for queries (e.g., bank, year, quarter).\n",
        "\n",
        "4. **Interactive Query Interface**\n",
        "   - Display a menu for users to choose an analysis type:\n",
        "     1. **Ask a Generic Question**: Retrieve and analyze documents based on a user-provided question and filters.\n",
        "     2. **Compare Quarters (Same Year)**: Compare performance between two quarters in the same year.\n",
        "     3. **Compare Quarters (Different Years)**: Compare performance across two years for specific quarters.\n",
        "     4. **Year-over-Year Comparison**: Analyze year-on-year performance for a specific bank.\n",
        "     5. **Analyze All Quarters in a Year**: Summarize and analyze performance for all quarters in a year.\n",
        "     6. **Analyze Sentiment for a Quarter**: Perform sentiment analysis for a specific quarter and bank.\n",
        "     7. **Summarize a Quarter**: Summarize financial performance for a given quarter and bank.\n",
        "     8. **Topic Modeling (BERTopic)**: Perform topic modeling on filtered documents using BERTopic.\n",
        "     9. **Topic Modeling (Gensim LDA)**: Perform topic modeling on filtered documents using Gensim LDA.\n",
        "    10. **Exit**: Exit the application.\n",
        "\n",
        "5. **Process User Selection**\n",
        "   - For each choice:\n",
        "     - Prompt user for relevant inputs (e.g., year, quarter, bank).\n",
        "     - Call the corresponding handler function (e.g., `handle_generic_query`, `handle_sentiment_single_quarter`).\n",
        "     - Display results using `display_wrapped_output`.\n",
        "\n",
        "6. **Error Handling**\n",
        "   - Log and display errors encountered during pipeline configuration or query handling.\n",
        "\n",
        "7. **Exit Application**\n",
        "   - Exit gracefully when the user selects the exit option."
      ],
      "metadata": {
        "id": "bKqc59WMGnys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to configure the RAG pipeline and start the interactive user interface.\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(\"RAGApplication\")\n",
        "    logger.info(\"Starting Transcript-Based Risk Analyzer\")\n",
        "\n",
        "    # Step 1: Initialize pipeline, tokenizer, etc.\n",
        "    try:\n",
        "        folder_path = \"\"\n",
        "        metadata_path = \"\"\n",
        "\n",
        "        display_title()\n",
        "        # Select model\n",
        "        print(\"\\nStep 1: Select a Language Model\")\n",
        "        selected_model = select_model()\n",
        "        model_token_limit = get_model_token_limit(selected_model)\n",
        "\n",
        "        # Configure pipeline\n",
        "        print(\"\\n[INFO] Configuring the RAG pipeline...\")\n",
        "        pipeline = configure_rag_pipeline(\n",
        "            folder_path=folder_path,\n",
        "            metadata_path=metadata_path,\n",
        "            model_name=selected_model,\n",
        "            embeddings_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "            model_token_limit=model_token_limit,\n",
        "            top_k=3,\n",
        "            max_new_tokens=500,\n",
        "            chunk_size=None,\n",
        "            chunk_overlap=200,\n",
        "            required_metadata_fields={\"year\", \"quarter\", \"bank\", \"source\", \"designation\", \"name\"},\n",
        "            persist_directory=\"qa_index\",\n",
        "            device=\"cuda\",\n",
        "        )\n",
        "\n",
        "        if pipeline is None:\n",
        "            raise ValueError(\"Pipeline configuration returned no results.\")\n",
        "\n",
        "        # Configure retriever\n",
        "        retriever = setup_retriever(pipeline[\"vector_store\"], top_k=3)\n",
        "        pipeline[\"retriever\"] = retriever  # Add retriever to pipeline for later use\n",
        "\n",
        "        logger.info(\"Pipeline configured successfully.\")\n",
        "        print(\"\\n[INFO] Pipeline configured successfully.\")\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(selected_model)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Pipeline configuration failed: {e}\")\n",
        "        print(f\"[ERROR] Pipeline configuration failed: {e}\")\n",
        "        return\n",
        "\n",
        "    # Helper function to collect filter inputs\n",
        "    def get_filters():\n",
        "        filters = {}\n",
        "        filter_bank = input(\"Do you want to filter by a specific bank? (yes/no): \").strip().lower()\n",
        "        if filter_bank == \"yes\":\n",
        "            filters[\"bank\"] = input(\"Enter the bank name: \").strip().lower()\n",
        "        year = input(\"Enter the year to filter (or press Enter to skip): \").strip()\n",
        "        if year:\n",
        "            filters[\"year\"] = year\n",
        "        quarter = input(\"Enter the quarter to filter (e.g., Q1, or press Enter to skip): \").strip()\n",
        "        if quarter:\n",
        "            filters[\"quarter\"] = quarter\n",
        "        return filters\n",
        "\n",
        "    # Step 2: Interactive Query Interface\n",
        "    try:\n",
        "        while True:\n",
        "            print('\\nRAG Based Analysis:')\n",
        "            print(\"-------------------\\n\")\n",
        "            print(\"\\n--- Query Selection Menu ---\")\n",
        "            print(\"1. Ask a generic question\")\n",
        "            print(\"2. Compare two quarters of the same year\")\n",
        "            print(\"3. Compare two quarters of different years\")\n",
        "            print(\"4. Year-over-year comparison\")\n",
        "            print(\"5. Analyze all quarters of a year\")\n",
        "            print(\"6. Analyze sentiment for a single quarter\")\n",
        "            print(\"7. Summarize a single quarter\")\n",
        "            print(\"8. Perform Topic Modeling with BERTopic\")\n",
        "            print(\"9. Perform Topic Modeling with Gensim LDA\")\n",
        "            print(\"10. Exit\") #adjust exit accordingly\n",
        "\n",
        "            choice = input(\"\\nSelect an option (1-10): \").strip()\n",
        "\n",
        "            if not choice.isdigit() or not (1 <= int(choice) <= 10):\n",
        "                print(\"[ERROR] Invalid option. Please enter a number between 1 and 10.\")\n",
        "                continue\n",
        "\n",
        "            choice = int(choice)\n",
        "\n",
        "            if choice == 1:\n",
        "                query = input(\"Enter your question: \").strip()\n",
        "                filters = get_filters()\n",
        "                answer = handle_generic_query(\n",
        "                    pipeline=pipeline,\n",
        "                    tokenizer=tokenizer,\n",
        "                    summarization_pipeline=pipeline[\"summarization_pipeline\"],\n",
        "                    model_token_limit=model_token_limit,\n",
        "                    filters=filters,\n",
        "                    query=query,\n",
        "                )\n",
        "            elif choice == 2:\n",
        "                year = input(\"Enter the year (e.g., 2023): \").strip()\n",
        "                quarter1 = input(\"Enter Quarter 1 (e.g., Q1): \").strip()\n",
        "                quarter2 = input(\"Enter Quarter 2 (e.g., Q2): \").strip()\n",
        "                bank = input(\"Enter the bank name: \").strip().lower()\n",
        "                query = \"Compare the financial performance of these quarters.\"\n",
        "                answer = handle_compare_quarters_same_year(\n",
        "                    pipeline=pipeline,\n",
        "                    tokenizer=tokenizer,\n",
        "                    model_token_limit=model_token_limit,\n",
        "                    year=year,\n",
        "                    quarter1=quarter1,\n",
        "                    quarter2=quarter2,\n",
        "                    bank=bank,\n",
        "                    query=query,\n",
        "                )\n",
        "            elif choice == 3:\n",
        "                year1 = input(\"Enter Year 1 (e.g., 2022): \").strip()\n",
        "                quarter1 = input(\"Enter Quarter 1 (e.g., Q1): \").strip()\n",
        "                year2 = input(\"Enter Year 2 (e.g., 2023): \").strip()\n",
        "                quarter2 = input(\"Enter Quarter 2 (e.g., Q2): \").strip()\n",
        "                bank = input(\"Enter the bank name: \").strip().lower()\n",
        "                query = \"Compare the performance of these quarters across years.\"\n",
        "                answer = handle_compare_quarters_diff_years(\n",
        "                    pipeline=pipeline,\n",
        "                    tokenizer=tokenizer,\n",
        "                    model_token_limit=model_token_limit,\n",
        "                    year1=year1,\n",
        "                    year2=year2,\n",
        "                    quarter1=quarter1,\n",
        "                    quarter2=quarter2,\n",
        "                    bank=bank,\n",
        "                    query=query,\n",
        "                )\n",
        "            elif choice == 4:\n",
        "                year1 = input(\"Enter Year 1 (e.g., 2022): \").strip()\n",
        "                year2 = input(\"Enter Year 2 (e.g., 2023): \").strip()\n",
        "                bank = input(\"Enter the bank name: \").strip().lower()\n",
        "                query = \"Provide a year-over-year performance comparison.\"\n",
        "                answer = handle_year_comparison(\n",
        "                    pipeline=pipeline,\n",
        "                    tokenizer=tokenizer,\n",
        "                    model_token_limit=model_token_limit,\n",
        "                    year1=year1,\n",
        "                    year2=year2,\n",
        "                    bank=bank,\n",
        "                    query=query,\n",
        "                )\n",
        "            elif choice == 5:\n",
        "                year = input(\"Enter the year (e.g., 2023): \").strip()\n",
        "                bank = input(\"Enter the bank name: \").strip().lower()\n",
        "                query = \"Analyze all quarters of the given year.\"\n",
        "                answer = handle_compare_quarters_same_year(\n",
        "                    pipeline=pipeline,\n",
        "                    tokenizer=tokenizer,\n",
        "                    model_token_limit=model_token_limit,\n",
        "                    year=year,\n",
        "                    quarter1=\"Q1\",\n",
        "                    quarter2=\"Q4\",  # Analyzing from Q1 to Q4\n",
        "                    bank=bank,\n",
        "                    query=query,\n",
        "                )\n",
        "            elif choice == 6:\n",
        "                year = input(\"Enter the year (e.g., 2023): \").strip()\n",
        "                quarter = input(\"Enter the quarter (e.g., Q1): \").strip()\n",
        "                bank = input(\"Enter the bank name: \").strip().lower()\n",
        "                query = \"Analyze the sentiment for this quarter.\"\n",
        "                answer = handle_sentiment_single_quarter(\n",
        "                    pipeline=pipeline,\n",
        "                    tokenizer=tokenizer,\n",
        "                    model_token_limit=model_token_limit,\n",
        "                    year=year,\n",
        "                    quarter=quarter,\n",
        "                    bank=bank,\n",
        "                    query=query,\n",
        "                )\n",
        "            elif choice == 7:\n",
        "                year = input(\"Enter the year (e.g., 2023): \").strip()\n",
        "                quarter = input(\"Enter the quarter (e.g., Q1): \").strip()\n",
        "                bank = input(\"Enter the bank name: \").strip().lower()\n",
        "                query = \"Summarize the performance for this quarter.\"\n",
        "                answer = handle_summarize_single_quarter(\n",
        "                    pipeline=pipeline,\n",
        "                    tokenizer=tokenizer,\n",
        "                    model_token_limit=model_token_limit,\n",
        "                    year=year,\n",
        "                    quarter=quarter,\n",
        "                    bank=bank,\n",
        "                    query=query,\n",
        "                )\n",
        "            elif choice == 8:\n",
        "                print(\"\\n[INFO] Performing Topic Modeling with BERTopic...\")\n",
        "                filters = get_filters()\n",
        "                filtered_docs = filter_documents(pipeline[\"documents\"], **filters)\n",
        "                if not filtered_docs:\n",
        "                    print(\"[ERROR] No documents matched the specified filters for BERTopic.\")\n",
        "                    continue\n",
        "                handle_bertopic(filtered_docs)\n",
        "                answer = \"BERTopic modeling completed successfully.\"\n",
        "            elif choice == 9:\n",
        "                print(\"\\n[INFO] Performing Topic Modeling with Gensim LDA...\")\n",
        "                filters = get_filters()\n",
        "                filtered_docs = filter_documents(pipeline[\"documents\"], **filters)\n",
        "                if not filtered_docs:\n",
        "                    print(\"[ERROR] No documents matched the specified filters for Gensim LDA.\")\n",
        "                    continue\n",
        "                handle_gensim_lda(filtered_docs, nlp_model=nlp_model, num_topics=6)\n",
        "                answer = \"Gensim LDA modeling completed successfully.\"\n",
        "            elif choice == 10:\n",
        "                print(\"Exiting the application. Goodbye!\")\n",
        "                break\n",
        "\n",
        "            display_wrapped_output(remove_repetitions(answer), width=100)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"An unexpected error occurred in the main function: {e}\")\n",
        "        print(f\"[ERROR] An unexpected error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "iJJOhNrgIlQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "bWzBgmTgwgpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main call"
      ],
      "metadata": {
        "id": "5SKHDM7SnsSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "_NsYINgjklbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Enhanced RAG Pipeline for future implementation\n"
      ],
      "metadata": {
        "id": "jbKIDX3ZVmVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_lda_topics_for_document(row, dictionary, lda_model):\n",
        "    if \"lda_topics\" not in row or not isinstance(row[\"lda_topics\"], str) or not row[\"lda_topics\"]:\n",
        "        logger.warning(f\"Skipping row with missing or empty 'lda_topics': {row}\")\n",
        "        return []\n",
        "\n",
        "    doc_bow = dictionary.doc2bow(row[\"lda_topics\"].split())  # Tokenized content\n",
        "    topic_distribution = lda_model.get_document_topics(doc_bow, minimum_probability=0.05)\n",
        "    return [f\"Topic {topic[0]}: {topic[1]:.2f}\" for topic in topic_distribution]\n",
        "\n",
        "def extract_lda_topics_for_context(context, dictionary, lda_model):\n",
        "    \"\"\"\n",
        "    Extract topics for a given context using the LDA model.\n",
        "\n",
        "    Args:\n",
        "        context (str): The text content to analyze.\n",
        "        dictionary (Dictionary): Gensim dictionary for the corpus.\n",
        "        lda_model (LdaModel): Trained Gensim LDA model.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of topics and their probabilities.\n",
        "    \"\"\"\n",
        "    if not isinstance(context, str) or not context.strip():\n",
        "        logger.warning(f\"Invalid or empty context: {context}\")\n",
        "        return []\n",
        "\n",
        "    doc_bow = dictionary.doc2bow(context.split())  # Tokenized content\n",
        "    topic_distribution = lda_model.get_document_topics(doc_bow, minimum_probability=0.05)\n",
        "    return [f\"Topic {topic[0]}: {topic[1]:.2f}\" for topic in topic_distribution]\n",
        "\n",
        "\n",
        "def configure_rag_pipeline_future(\n",
        "    folder_path: str,\n",
        "    metadata_path: str,\n",
        "    context: str = None,\n",
        "    model_name: str = \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    embeddings_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_token_limit: int = 4096,\n",
        "    top_k: int = 3,\n",
        "    max_new_tokens: int = 500,\n",
        "    chunk_size: int = None,\n",
        "    chunk_overlap: int = 200,\n",
        "    required_metadata_fields: Set[str] = None,\n",
        "    persist_directory: str = \"qa_index\",\n",
        "    device: str = \"cuda\",\n",
        "    num_topics: int = 3,\n",
        "    chain_type: str = \"stuff\",\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Configures a Retrieval-Augmented Generation (RAG) pipeline with QA chains, LDA, and other components.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing all pipeline components.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Validate file paths\n",
        "        if not os.path.exists(folder_path):\n",
        "            raise FileNotFoundError(f\"The specified folder path does not exist: {folder_path}\")\n",
        "        if not os.path.exists(metadata_path):\n",
        "            raise FileNotFoundError(f\"The metadata file does not exist: {metadata_path}\")\n",
        "\n",
        "        # Step 2: Calculate dynamic chunk size\n",
        "        if chunk_size is None:\n",
        "            prompt_tokens = 200\n",
        "            token_budget = model_token_limit - prompt_tokens - max_new_tokens\n",
        "            chunk_size = min(2000, token_budget // 2)\n",
        "            logger.info(f\"Dynamic chunk size set to: {chunk_size}\")\n",
        "\n",
        "        # Step 3: Adjust chunk overlap if necessary\n",
        "        if chunk_overlap is None or chunk_overlap >= chunk_size:\n",
        "            chunk_overlap = chunk_size // 5\n",
        "            logger.info(f\"Dynamic chunk overlap set to: {chunk_overlap}\")\n",
        "\n",
        "        # Step 4: Load and preprocess documents\n",
        "        logger.info(\"Loading and preprocessing documents...\")\n",
        "        enriched_documents, document_chunks = load_and_preprocess_data(\n",
        "            folder_path=folder_path,\n",
        "            metadata_path=metadata_path,\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            required_metadata_fields=required_metadata_fields,\n",
        "        )\n",
        "\n",
        "        # Step 5: Initialize embeddings and vector store\n",
        "        logger.info(\"Initializing embeddings and vector store...\")\n",
        "        vector_store = initialize_embeddings_and_vector_store(\n",
        "            chunks=document_chunks,\n",
        "            embeddings_model_name=embeddings_model_name,\n",
        "            persist_directory=persist_directory,\n",
        "            use_existing=True,\n",
        "        )\n",
        "\n",
        "        # Step 6: Setup retriever and language model\n",
        "        retriever = setup_retriever(vector_store, top_k=top_k)\n",
        "        llm_pipeline = load_model(model_name, max_new_tokens=max_new_tokens, device=device)\n",
        "\n",
        "        # Step 7: Load and validate metadata\n",
        "        metadata_df = pd.read_csv(metadata_path)\n",
        "        if metadata_df.empty:\n",
        "            raise ValueError(\"Metadata file is empty. Ensure the metadata CSV is correctly populated.\")\n",
        "        logger.info(f\"Loaded metadata with {len(metadata_df)} entries.\")\n",
        "\n",
        "        # Step 8: Initialize summarization and sentiment pipelines\n",
        "        summarization_pipeline = hf_pipeline(\n",
        "            \"summarization\", model=\"sshleifer/distilbart-cnn-6-6\", device=0 if device == \"cuda\" else -1\n",
        "        )\n",
        "        sentiment_pipeline = hf_pipeline(\n",
        "            \"sentiment-analysis\", model=\"yiyanghkust/finbert-tone\", device=0 if device == \"cuda\" else -1\n",
        "        )\n",
        "\n",
        "        # Step 9: Perform LDA on metadata\n",
        "        lda_model_metadata, dictionary_metadata, topics_metadata = None, None, None\n",
        "        try:\n",
        "            logger.info(\"Performing LDA on metadata...\")\n",
        "            corpus = [doc.page_content for doc in enriched_documents if doc.page_content]\n",
        "            processed_corpus = preprocess_corpus_gensim(corpus, custom_stopwords=None, redundant_terms=None)\n",
        "            dictionary_metadata = Dictionary(processed_corpus)\n",
        "            bow_corpus_metadata = [dictionary_metadata.doc2bow(doc) for doc in processed_corpus]\n",
        "            lda_model_metadata = LdaModel(\n",
        "                corpus=bow_corpus_metadata,\n",
        "                id2word=dictionary_metadata,\n",
        "                num_topics=num_topics,\n",
        "                random_state=42,\n",
        "                passes=10,\n",
        "                iterations=50,\n",
        "            )\n",
        "            topics_metadata = extract_topics_from_lda(lda_model_metadata, bow_corpus_metadata, dictionary_metadata)\n",
        "            metadata_df[\"lda_topics\"] = topics_metadata\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error performing LDA on metadata: {e}\")\n",
        "            metadata_df[\"lda_topics\"] = None\n",
        "\n",
        "        # Step 10: Perform LDA on context\n",
        "        lda_model_context, dictionary_context, topics_context = None, None, None\n",
        "        if context:\n",
        "            try:\n",
        "                logger.info(\"Performing LDA on context...\")\n",
        "                processed_context = preprocess_context(context, nlp_model=None)\n",
        "                dictionary_context = Dictionary([processed_context])\n",
        "                bow_corpus_context = [dictionary_context.doc2bow(processed_context)]\n",
        "                lda_model_context = LdaModel(\n",
        "                    corpus=bow_corpus_context,\n",
        "                    id2word=dictionary_context,\n",
        "                    num_topics=num_topics,\n",
        "                    random_state=42,\n",
        "                    passes=10,\n",
        "                    iterations=50,\n",
        "                )\n",
        "                topics_context = extract_lda_topics_for_context(context, dictionary_context, lda_model_context)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error performing LDA on context: {e}\")\n",
        "\n",
        "        # Step 11: Initialize QA Chains\n",
        "        qa_chains = None\n",
        "        try:\n",
        "            logger.info(\"Initializing QA chains...\")\n",
        "            qa_chains = initialize_chains(llm_pipeline, chain_type=chain_type)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing QA chains: {e}\")\n",
        "\n",
        "        # Step 12: Return pipeline components\n",
        "        logger.info(\"RAG pipeline configured successfully.\")\n",
        "        return {\n",
        "            \"retriever\": retriever,\n",
        "            \"vector_store\": vector_store,\n",
        "            \"llm_pipeline\": llm_pipeline,\n",
        "            \"metadata_df\": metadata_df,\n",
        "            \"documents\": enriched_documents,\n",
        "            \"summarization_pipeline\": summarization_pipeline,\n",
        "            \"sentiment_pipeline\": sentiment_pipeline,\n",
        "            \"lda_model_metadata\": lda_model_metadata,\n",
        "            \"lda_topics_metadata\": topics_metadata,\n",
        "            \"lda_model_context\": lda_model_context,\n",
        "            \"lda_topics_context\": topics_context,\n",
        "            \"qa_chains\": qa_chains,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Pipeline configuration failed: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "iC1PZ5EPWX3j"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "qjtoeCRRDDA0",
        "EUxV-Ea-HuNY",
        "fAxYdyz-He30"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}